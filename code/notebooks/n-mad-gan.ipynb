{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gen = 2 #number of generators\n",
    "latent_dim = 256 #dimention of input noise\n",
    "batch_size = 256 #number of batches\n",
    "size_dataset = 60_000 #size MNIST dataset - 60_000\n",
    "epochs = 2\n",
    "steps_per_epoch = (size_dataset//batch_size)//n_gen\n",
    "type = 'no-stack'\n",
    "\n",
    "import datetime \n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "dir_name = f\"models/MNIST_{n_gen}-gen_{epochs}-ep_{type}-type_{current_date}\" #location to save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder created at: models/MNIST_2-gen_2-ep_no-stack-type_2024-12-05\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check if the folder exists, if not, create it\n",
    "if not os.path.exists(dir_name):\n",
    "    os.makedirs(dir_name)\n",
    "    print(f\"Folder created at: {dir_name}\")\n",
    "else:\n",
    "    print(f\"Folder already exists at: {dir_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import mixture\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Input, Dense, Dropout, LeakyReLU, \n",
    "                                     ReLU, Conv2D,Conv2DTranspose, Flatten,\n",
    "                                     Reshape, BatchNormalization)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Done\n"
     ]
    }
   ],
   "source": [
    "# for saving GIF\n",
    "import imageio\n",
    "import glob\n",
    "print(\"Import Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To see if we have a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Using a GPU\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "if tf.test.gpu_device_name() == '/device:GPU:0':\n",
    "    print(\"Using a GPU\")\n",
    "else:\n",
    "    print(\"Using a CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(f\"GPUs available: {tf.config.list_physical_devices('GPU')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aquire dataset MNIST\n",
    "## generate latent samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Shape of data\")\n",
    "train_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_probability import distributions as tfd\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, batch_size, n_gen):\n",
    "    # Multivariate normal diagonal distribution\n",
    "    mvn = tfd.MultivariateNormalDiag(\n",
    "        loc=[0]*latent_dim,\n",
    "        scale_diag=[1.0]*latent_dim)\n",
    "\n",
    "    noise = []\n",
    "    for i in range(n_gen):\n",
    "        # Some samples from MVN\n",
    "        x_input = mvn.sample(batch_size)\n",
    "        noise.append(x_input)\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generator related\n",
    "## monitor, loss\n",
    "## plotting funcitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "# Function to create a GIF from a list of image paths\n",
    "def create_gif(image_folder, output_gif, duration=500):\n",
    "    image_files = sorted([os.path.join(image_folder, file) for file in os.listdir(image_folder) if file.endswith('.png')])\n",
    "\n",
    "    frames = []\n",
    "    for image_name in image_files:\n",
    "        img = Image.open(image_name)\n",
    "        frames.append(img)\n",
    "\n",
    "    # Save the frames as an animated GIF\n",
    "    frames[0].save(output_gif, format='GIF', append_images=frames[1:], save_all=True, duration=duration, loop=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_training_history(history, save: bool = True):\n",
    "    # Extract losses from history\n",
    "    history_dict = history.history\n",
    "    generator_losses = []\n",
    "    discriminator_loss = None\n",
    "    \n",
    "    # Separate generator losses and discriminator loss\n",
    "    for key in history_dict.keys():\n",
    "        if 'g_loss' in key:\n",
    "            generator_losses.append((key, history_dict[key]))\n",
    "        elif key == 'd_loss':\n",
    "            discriminator_loss = history_dict[key]\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot generator losses\n",
    "    for gen_loss_key, gen_loss_values in generator_losses:\n",
    "        plt.plot(gen_loss_values, label=gen_loss_key)\n",
    "    \n",
    "    # Plot discriminator loss\n",
    "    if discriminator_loss is not None:\n",
    "        plt.plot(discriminator_loss, label='d_loss', linewidth=2, linestyle='--', color='black')\n",
    "    \n",
    "    plt.title('Training Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "    if save: \n",
    "        plt.savefig(f'{dir_name}/image_at_epoch_{(epoch + 1):04}.png', dpi=200, format=\"png\")\n",
    "\n",
    "\n",
    "def plot_generators_examples(\n",
    "    n_rows: int, n_cols: int, \n",
    "    random_latent_vectors: list, \n",
    "    data, generators: list,  \n",
    "    dir_name: str, \n",
    "    epoch: int, \n",
    "    save: bool = False, show: bool = True,\n",
    ") -> None: \n",
    "    # Create a figure and a grid of subplots\n",
    "    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(12, 8))\n",
    "    fig.suptitle(f'Epoch: {epoch}', fontsize=20)\n",
    "    # Flatten the axes array to iterate over individual subplots\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Iterate over the subplots\n",
    "    for i, ax in enumerate(axes):\n",
    "        # Calculate the current row based on index\n",
    "        current_row = i // n_cols\n",
    "        \n",
    "        # Determine if we're plotting real or generated data\n",
    "        if (i + 1) % n_cols == 0: \n",
    "            # Plot real data\n",
    "            if current_row < len(data):\n",
    "                ax.imshow((data[current_row, :, :,] * 127.5 + 127.5) / 255, cmap='gray')\n",
    "                ax.set_title(\"REAL\")\n",
    "            else:\n",
    "                print(f\"Skipping real data plot for row {current_row}: Index out of bounds.\")\n",
    "        else: \n",
    "            # Plot generated data\n",
    "            generator_index = i % (n_cols - 1)\n",
    "            generated_sample = generators[generator_index](random_latent_vectors[generator_index])\n",
    "            ax.imshow((generated_sample[current_row, :, :,] * 127.5 + 127.5) / 255, cmap='gray')\n",
    "            ax.set_title(f\"FAKE (Gen {generator_index + 1})\")\n",
    "        \n",
    "        # Turn off axis labels for clarity\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Adjust layout and spacing\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if save: \n",
    "        plt.savefig(f'{dir_name}/image_at_epoch_{(epoch + 1):04}.png', dpi=200, format=\"png\")\n",
    "    if show: \n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loss function for the generators based on the MAD_GAN paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generators_loss_function(y_true, y_pred): \n",
    "    logarithm = -tf.math.log(y_pred[:,-1] + 1e-15)\n",
    "    return tf.reduce_mean(logarithm, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, \n",
    "                 random_latent_vectors, \n",
    "                 data, \n",
    "                 n_classes, \n",
    "                 latent_dim = 128,  \n",
    "                 dir_name = 'Model', \n",
    "                 generate_after_epochs: int = 10\n",
    "                ):\n",
    "        self.data = data[0:10]\n",
    "        self.random_latent_vectors = random_latent_vectors\n",
    "        self.latent_dim = latent_dim\n",
    "        self.dir_name = dir_name\n",
    "        self.n_classes = n_classes\n",
    "        self.generate_after_epochs = generate_after_epochs\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % self.generate_after_epochs == 0: \n",
    "            plot_generators_examples(\n",
    "                n_rows=len(self.model.generators), \n",
    "                n_cols=len(self.model.generators) + 1, \n",
    "                dir_name=self.dir_name, \n",
    "                random_latent_vectors=self.random_latent_vectors, \n",
    "                data=self.data,\n",
    "                generators=self.model.generators, \n",
    "                epoch=epoch,\n",
    "                save=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define Discriminator \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone discriminator model\n",
    "def define_discriminator(n_gen):\n",
    "    inp = Input(shape=(28, 28, 1))\n",
    "\n",
    "    x = Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1])(inp)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Conv2D(128, (5, 5), strides=(2, 2), padding='same')(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    out = Dense(n_gen + 1, activation = 'softmax')(x)\n",
    "\n",
    "    model = Model(inp, out, name=\"Discriminator\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12544"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7*7*256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_generators(n_gen, latent_dim, class_labels):\n",
    "    dens = Dense(units=7*7*256, use_bias=False, input_shape=(latent_dim,))\n",
    "    batchnorm0 = BatchNormalization()\n",
    "    rel0 = LeakyReLU()\n",
    "    reshape0 = Reshape([7,7,latent_dim])\n",
    "\n",
    "    con2dt1 = Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False)\n",
    "    batchnorm1 = BatchNormalization()\n",
    "    rel1 = LeakyReLU()\n",
    "\n",
    "    con2dt2 = Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False)\n",
    "    batchnorm2 = BatchNormalization()\n",
    "    rel2 = LeakyReLU()\n",
    "\n",
    "    models = []\n",
    "    for label in range(n_gen):\n",
    "        input = Input(shape=(latent_dim,), dtype = tf.float64, name=f\"input_{label}\")\n",
    "        x = dens(input)\n",
    "        x = batchnorm0(x)\n",
    "        x = rel0(x)\n",
    "        x = reshape0(x)\n",
    "\n",
    "        x = con2dt1(x)\n",
    "        x = batchnorm1(x)\n",
    "        x = rel1(x)\n",
    "        \n",
    "        x = con2dt2(x)\n",
    "        x = batchnorm2(x)\n",
    "        x = rel2(x)\n",
    "        \n",
    "        x = Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')(x)\n",
    "        \n",
    "        models.append(Model(input, x, name = f\"generator{label}\"))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# defining the N-MAD GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADGAN(tf.keras.Model):\n",
    "    def __init__(self, discriminator, generators, latent_dim, n_gen):\n",
    "        super(MADGAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generators = generators\n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_gen = n_gen\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
    "        super(MADGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "\n",
    "    def train_step(self, data): \n",
    "        X = data\n",
    "        \n",
    "        batch_size = tf.shape(X)[0]\n",
    "        random_latent_vectors = generate_latent_points(self.latent_dim, batch_size//self.n_gen, self.n_gen)\n",
    "        \n",
    "        print(self.latent_dim, batch_size//self.n_gen, self.n_gen)\n",
    "        # Decode them to fake generator output\n",
    "        x_generator = []\n",
    "        for g in range(self.n_gen):\n",
    "            x_generator.append(self.generators[g](random_latent_vectors[g]))\n",
    "        \n",
    "        # Combine them with real samples\n",
    "        combined_samples = tf.concat([x_generator[g] for g in range(self.n_gen)] + \n",
    "                                     [X], \n",
    "                                     axis=0\n",
    "                                     )\n",
    "        # Assemble labels discriminating real from fake samples\n",
    "        labels = tf.concat(\n",
    "            [\n",
    "                tf.one_hot(g * tf.ones(batch_size//self.n_gen, dtype=tf.int32), self.n_gen + 1) \n",
    "                for g in range(self.n_gen)\n",
    "            ] + \n",
    "            [tf.one_hot(self.n_gen * tf.ones(batch_size, dtype=tf.int32), self.n_gen + 1)], \n",
    "            axis=0\n",
    "        )\n",
    "\n",
    "        # Add random noise to the labels. important trick\n",
    "        labels += 0.05 * tf.random.uniform(shape = tf.shape(labels), minval = -1, maxval = 1)\n",
    "        \n",
    "        #######################\n",
    "        # Train Discriminator #\n",
    "        #######################\n",
    "        \n",
    "        # make weights in the discriminator trainable\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Discriminator forward pass\n",
    "            predictions = self.discriminator(combined_samples)\n",
    "            \n",
    "            # Compute the loss value\n",
    "            d_loss = self.d_loss_fn(labels, predictions)\n",
    "            \n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        # Update weights\n",
    "        self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))\n",
    "        \n",
    "        #######################\n",
    "        #   Train Generator   #\n",
    "        #######################\n",
    "        \n",
    "        # Assemble labels that say are \"all real samples\" to try to fool the disc during gen training\n",
    "        misleading_labels =  tf.one_hot(self.n_gen * tf.ones(batch_size//self.n_gen, dtype=tf.int32), self.n_gen + 1)\n",
    "        g_loss_list = []\n",
    "        fake_image = []\n",
    "        \n",
    "        for g in range(self.n_gen):\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Generator[g] and discriminator forward pass\n",
    "                predictions = self.discriminator(self.generators[g](random_latent_vectors[g]))\n",
    "                \n",
    "                # Compute the loss value\n",
    "                g_loss = self.g_loss_fn(misleading_labels, predictions)\n",
    "                \n",
    "            # Compute gradients\n",
    "            grads = tape.gradient(g_loss, self.generators[g].trainable_weights)\n",
    "            # Update weights\n",
    "            self.g_optimizer[g].apply_gradients(zip(grads, self.generators[g].trainable_weights))\n",
    "            g_loss_list.append(g_loss)\n",
    "            \n",
    "        mydict = {f\"g_loss{g}\": g_loss_list[g] for g in range(self.n_gen)}\n",
    "        mydict.update({\"d_loss\": d_loss})\n",
    "        return mydict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_func(random_state = None):\n",
    "    (train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "    # train_images = tf.image.resize(train_images, [32,32])\n",
    "    train_images = (train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]\n",
    "\n",
    "    # Convert to stacked-mnist(rgb images)\n",
    "    #t1 = tf.random.shuffle(train_images, seed = 10)\n",
    "    #t2 = tf.random.shuffle(train_images, seed = 20)\n",
    "    #train_images = tf.concat([train_images, t1, t2], axis=-1)\n",
    "    \n",
    "    return train_images, np.unique(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Discriminator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 14, 14, 64)        1664      \n",
      "                                                                 \n",
      " leaky_re_lu_15 (LeakyReLU)  (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 7, 7, 128)         204928    \n",
      "                                                                 \n",
      " leaky_re_lu_16 (LeakyReLU)  (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 6272)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 3)                 18819     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 225,411\n",
      "Trainable params: 225,411\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"generator0\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_0 (InputLayer)        [(None, 256)]             0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 12544)             3211264   \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 12544)            50176     \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_17 (LeakyReLU)  (None, 12544)             0         \n",
      "                                                                 \n",
      " reshape_3 (Reshape)         (None, 7, 7, 256)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_12 (Conv2D  (None, 7, 7, 128)        819200    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 7, 7, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_18 (LeakyReLU)  (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_13 (Conv2D  (None, 14, 14, 64)       204800    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 14, 14, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_19 (LeakyReLU)  (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_14 (Conv2D  (None, 28, 28, 1)        1600      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,287,808\n",
      "Trainable params: 4,262,336\n",
      "Non-trainable params: 25,472\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Changing numpy dataset to tf.DATASET type and Shuffling dataset for training\n",
    "data, unique_labels = dataset_func() \n",
    "dataset = tf.data.Dataset.from_tensor_slices(data) \n",
    "dataset = dataset.repeat().shuffle(10 * size_dataset, reshuffle_each_iteration=True).batch(n_gen * batch_size, drop_remainder=True)\n",
    "\n",
    "# Creating Discriminator and Generator\n",
    "discriminator = define_discriminator(n_gen)\n",
    "discriminator.summary()\n",
    "generators = define_generators(n_gen, latent_dim, class_labels=unique_labels)\n",
    "generators[0].summary()\n",
    "\n",
    "# creating MADGAN\n",
    "madgan = MADGAN(discriminator = discriminator, generators = generators, \n",
    "                latent_dim = latent_dim, n_gen = n_gen)\n",
    "\n",
    "madgan.compile(\n",
    "    d_optimizer = Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    g_optimizer = [Adam(learning_rate=1e-4, beta_1=0.5) for g in range(n_gen)],\n",
    "    d_loss_fn = CategoricalCrossentropy(),\n",
    "    g_loss_fn = Generators_loss_function\n",
    ")\n",
    "\n",
    "checkpoint_filepath = f'{dir_name}\\checkpoint.weights.h5'\n",
    "random_latent_vectors = generate_latent_points(latent_dim = latent_dim, batch_size = 11, n_gen = n_gen)\n",
    "\n",
    "my_callbacks = [\n",
    "    GANMonitor(\n",
    "        random_latent_vectors=random_latent_vectors, \n",
    "        data = data, \n",
    "        n_classes=len(unique_labels), \n",
    "        latent_dim = latent_dim, \n",
    "        dir_name = dir_name, \n",
    "    ),    \n",
    "    # This callback is for Saving the model\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_filepath , save_freq = 10 , save_weights_only = True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:From C:\\Users\\NiXoN\\miniconda3\\envs\\local_thesis_conda\\lib\\site-packages\\tensorflow_probability\\python\\distributions\\distribution.py:342: calling MultivariateNormalDiag.__init__ (from tensorflow_probability.python.distributions.mvn_diag) with scale_identity_multiplier is deprecated and will be removed after 2020-01-01.\n",
      "Instructions for updating:\n",
      "`scale_identity_multiplier` is deprecated; please combine it into `scale_diag` directly instead.\n",
      "256 Tensor(\"floordiv_1:0\", shape=(), dtype=int32) 2\n",
      "256 Tensor(\"floordiv_1:0\", shape=(), dtype=int32) 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# # Loading previous saved model for resume training\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# if os.path.exists(checkpoint_filepath):\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#     madgan.load_weights(checkpoint_filepath)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#     print(f\"Continue to train model from path: {checkpoint_filepath}\")\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmadgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmy_callbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\local_thesis_conda\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\local_thesis_conda\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\local_thesis_conda\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\local_thesis_conda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\local_thesis_conda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:980\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[0;32m    977\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    978\u001b[0m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[0;32m    979\u001b[0m     \u001b[38;5;66;03m# stateless function.\u001b[39;00m\n\u001b[1;32m--> 980\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    982\u001b[0m   _, _, filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    983\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn\u001b[38;5;241m.\u001b[39m_function_spec\u001b[38;5;241m.\u001b[39mcanonicalize_function_inputs(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    984\u001b[0m           \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\local_thesis_conda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\local_thesis_conda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\local_thesis_conda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\local_thesis_conda\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # Loading previous saved model for resume training\n",
    "# if os.path.exists(checkpoint_filepath):\n",
    "#     madgan.load_weights(checkpoint_filepath)\n",
    "#     print(f\"Continue to train model from path: {checkpoint_filepath}\")\n",
    "\n",
    "# train the model\n",
    "history = madgan.fit(dataset, epochs = 2, steps_per_epoch = steps_per_epoch, verbose = 1, callbacks = my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_gif(image_folder=dir_name, output_gif=dir_name + f\"/output.gif\", duration=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((generators[0](random_latent_vectors[0]) * 127.5 + 127.5)  / 127.5) * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
