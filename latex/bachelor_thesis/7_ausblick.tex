\section{Ausblick}\label{ausblick}

% - Set 2 Frage 1
% 	- Würde der Agent jemals ein unbekanntes Level schaffen? Er müsste verstehen, dass er sich merken muss, wo er schon war. Würde ein ausreichend langes Training dafür sorgen, dass der Agent die Level konsistent schafft? Dann vielleicht sogar besser, als ein Agent der im Training diese Information zur Verfügung hatte? 

%- Abweichungen vom Pixel-Stream  
%	- Wie wirken sich Änderungen im Pixel-Stream aus? 
%	- Wie ist die Korrelation zwischen Änderungen in der Pixel-Verteilung mit der allgemeinen Performance zusammen?
	
Über die Ausarbeitung der Experimente sind an einigen Stellen Fragen aufgekommen, welche hier aufgegriffen werden. Die Experimente in Unterkapitel \ref{subsec:absch_EXP_durch_reproduktion_generalisierung} zeigen, dass eine Generalisierungslücke entsteht, wenn man auf einer beschränkten Anzahl an Leveln trainiert. Trainiert man einen Agenten für dieselbe Dauer mit einer unbeschränkten Anzahl an Trainings-Leveln, wird diese Lücke geschlossen. Da alle Experimente dieser Arbeit lediglich mit einem Seed für die prozedurale Generierung ausgeführt sind, kann die Aussage, dass die prozedurale Generierung hilft die Lücke zu schließen nur für diesen einen Seed beantwortet werden. Ein ähnliches Experiment über mehrere Seeds hinweg würde diese Aussage bekräftigen und die Hypothese der Autoren von Procgen unterstützen, dass die Procgen-Environments gut geeignet sind, um die Generalisierung und Sample Efficiency eines Algorithmus oder einer Architektur zu evaluieren. 

In den Experimenten zur Beantwortung der Fragen aus Set 2, Punkt 2 (\ref{subsec_EXP_durch_fragen}), ist der Agent von Abbildung \ref{fig:grph_green_80Mio_inflvl_15act_Training_evalAsTraining} in der Lage mit einer ihm unbekannten Orb-Farbe nahezu identische Rewards zu erzielen, wie mit der ihm bekannten Farbe. Diese Ergebnisse werden im darauffolgenden Experiment unter Bedingungen untersucht, die diesen Fakt genauer darstellen sollen. Die Ergebnisse können jedoch nicht reproduziert werden. Das bringt die Frage auf, ob nahezu identischen Rewards mit zwei Orb-Farben lediglich Zufall sind oder ob hier eine unbekannte Korrelation zwischen den beiden Farben besteht, die dem Agenten geholfen hat. Es wäre auch denkbar, dass eine farbunabhängige Korrelation zwischen den Durchläufen besteht, die dem Agenten zu dieser Performance verhilft.

Das Experiment in \ref{par:durch_EXP_farbÄnd_Speichervermögen} untersucht, wie viele Level mit der verwendeten Architektur auswendig gelernt werden können und welche Auswirkungen die visuelle Information der Orbs dabei hat. Hierbei zeigt sich der Trend, dass die Agenten, welche mit visuellen Orb-Information trainiert sind zwar konstant mehr Rewards bekommen, sich jedoch den Rewards der Agenten ohne visuelle Orb-Information mehr und mehr anpassen. Dieses Experiment in größerem Maßstab könnte zeigen, ob sich der Reward des Agenten mit visueller Information der Orbs, asymptotisch an die Rewards der anderen Agenten annähert oder ob die visuelle Orb-Information eine höhere Untergrenze setzt. Das würde die Hypothese aus \ref{sub_absch_EXP_durch_serie1_orbsMaskieren} unterstützen, dass die Orbs eine notwendige bzw. sehr hilfreiche visuelle Information im Chaser-Environment darstellen. 

Die Experimente zur visuellen semantischen Invertierung (\ref{absch_EXP_durch_serie1}) zeigen eindeutig, dass diese Veränderung die Performance der Agenten auf die des Random-Agenten abfallen lässt. Videoanalysen der Evaluation beider Invertierungen zeigen, dass die Agenten zu keinem Zeitpunkt zielführende Entscheidungen treffen können. Daraus kann man schließen, dass die Objekt-Detektion und Objekt-Erkennung der Agenten stark auf die jeweiligen Texturen und Farben der vertauschten Sprites overfitted und die Mechanik der Spielelemente wenig bis keine Auswirkung auf Erkennung und Detektion der Spielelemente hat. Hier ist interessant, ob die Änderung der Pixel-Verteilung von Training zu Evaluation durch mindern der visuellen Komplexität des Agenten-Inputs, semantische Invertierungen in der Evaluation erlauben würde. Eine Möglichkeit die visuelle Komplexität zu mindern wäre alle Texturen in Chaser durch einfarbige, solide Farben zu ersetzten. Die Feature-Extraction-Pipeline hätte somit lediglich farbliche Unterscheidung zwischen den Objekten zur Verfügung. Das könnte der Pipeline erlauben Features aus anderen Information, wie bspw. Bewegungsmuster oder Mechaniken von Spielelementen zu extrahieren.

Des Weiteren kann man hinterfragen, ob der Agent des Experiments zur semantischen Invertierung nur scheitert, da die Änderung in der Evaluation unbekannt ist. Darauf aufbauend kann man untersuchen, ob ein Agent mit ausreichend Zeitschritten im Training jede Änderung des Environments lernen kann. Würde man in Chaser nach jeder Episode dem Orb eine zufällige andere Farbe zuweisen, kann man hiermit erneut untersuchen, ob der Agent die Evaluation mit mehreren Orb-Farben besser besteht als in dieser Arbeit. 


\newpage