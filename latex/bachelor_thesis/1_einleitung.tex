\section{Einleitung \& Motivation}\label{einleitung}
\pagestyle{fancy}
%\setcounter{page}{1}
Die Forschung rund um Reinforcement Learning (RL) hat in den letzten Jahren stark an Interesse gewonnen. Eine Suche des Begriffs ‘Reinforcement Learning’ auf Google trends zeigt, dass sich die Häufigkeit der Anfragen dieses Begriffs seit 2016 knapp verdreifacht hat.
Gegenwärtige Benchmark-Umgebungen wie openAI’s Gym \cite{brockman2016openAI_gym} helfen den Fortschritt in der Forschung weiter voranzutreiben. Ein Weiterer, häufig verwendeter Simulator ist das Atari Learning Environment (ALE) \cite{bellemare2013arcade}. Das ALE bietet signifikante Forschungs-Aufgaben für RL, model-learning, model-based planning, imitation learning, transfer learning und intrinsic motivation \cite{bellemare2013arcade}. Neben den Vorteilen, die eine solche Benchmark-Umgebung und die darin befindlichen Atari Spiele mit sich bringen, haben diese Umgebungen immer noch entscheidende Nachteile: Die Level in einigen dieser Umgebungen sind händisch erstellt worden. Somit gibt es nur eine begrenzte Anzahl an Szenarien, in denen trainiert werden kann. Dieser Fakt führt bei der Arbeit mit RL-Agenten oft zu schlechterer Generalisierung und instabilen Policies \cite{zhang2018natural}.
Das Framework von Procgen adressiert das Problem der Generalisierung durch prozedurale Erstellung der Spielumgebung. Der prozedurale Anteil steuert dabei Entscheidungen wie die Logik des Levellayouts, die Position und Spawn-Zeiten von Spielelementen, die Auswahl der Game-Assets und weitere \cite{cobbe2019procgen}.

Im Paper \cite{dubey2018investigating} untersuchen die Autoren, warum Menschen im Lösen komplexer Videospiele so gut sind. Genauer untersuchen sie die Rolle von Vorwissen, welches hilfreich scheint, um eine schnelle und effiziente Exploration eines Frames oder eines Levels zu ermöglichen. Die Arbeit der Autoren dient als grundlegende Inspiration für diese Arbeit. Über den Verlauf ihrer Experimente hinweg verändern sie die visuelle Darstellung ihres Spiels bis zu einem Punkt, an dem bspw. die Ähnlichkeit von gleichen Objekten vollständig maskiert ist. Die in der Arbeit \cite{dubey2018investigating} erstellten Level werden von Menschen und einem RL-Agenten gespielt und ihre Leistungen werden verglichen. Die Level für den Agenten sind aufgrund der Komplexität des gewählten Spiels vereinfacht.

In der vorliegenden Arbeit wird untersucht, wie sich visuelle Augmentation des Environments auf die durch Procgen geförderte Generalisierung und den allgemeinen Erfolg trainierter Agenten auswirkt. Hierfür wird ein Environment von Procgen mit dem Titel \emph{Chaser} \cite{cobbe2019procgen}~[S.15 A.6] herangezogen. Dieses Spiel stellt ein Replika des Atari-Klassikers \dq Ms. Pac-Man\dq{} dar. 

Die Experimente sind in drei verschiedene Sets eingeteilt. Das erste Set überprüft die Reproduzierbarkeit der Ergebnisse aus dem Procgen-Paper und untersucht die Auswirkung prozeduraler Level-Generierung auf die Performance eines Agenten.

Das zweite Set beschäftigt sich mit visuellen Veränderungen der Farbe der Orbs und des Hintergrundes (BG), aber auch mit der Maskierung von Information, die im Training gegeben war. So werden in diesem Set z.B. Agenten in der Evaluation mit Herausforderungen konfrontiert wie: die Farbe der Orbs ist eine andere, als im Training oder die Orbs werden überhaupt nicht dargestellt.

Das dritte Set befasst sich ausschließlich mit der semantischen Invertierung von visueller Information unterschiedlicher Spielelemente. Hierbei werden bspw. in der Evaluation die Sprites von kleinen Orbs und Mauern oder großen Orbs und Gegnern vertauscht und somit die Semantik optisch invertiert. Ein Bild des Environments kann Abbildung \ref{fig:pic_chaserGros} entnommen werden. Kleine Orbs sind hier in grün und große sind gelb. Die Gegner sind die grünen Blobs mit drei Stacheln. Die Mauern sind grau. Diese Abbildung zeigt das Spiel in seiner unveränderten Form. 

Die ausgeführten Experimente finden in Anlehnung an die in \cite{dubey2018investigating} durchgeführten Experimenten statt. Besonders Änderungen der visuellen Semantik und die daraus resultierende Änderung der Erscheinung von Objekten, sollte die verwendete IMPALA-Architektur \cite{espeholt2018impala}, aufgrund des vorangeschaltenen CNN, vor komplexe Aufgaben stellen. 


\paragraph{Ziel der Arbeit}\label{ziel_der_arbeit}
Ziel dieser Arbeit ist die Untersuchung der Generalisierung des eingesetzten IMPALA-Netzwerks \ref{absch_setup_impala} im Zusammenspiel mit dem verwendeten PPO Algorithmus \ref{absch_RL_ppo},in der Procgen-Umgebung \cite{cobbe2019procgen}. Dies geschieht anhand unterschiedlicher visueller Augmentationen des Trainings- und/oder der Evaluations-Environments. Genauer wird die durch Procgen \ref{absch_EXP_procgen} geförderte Generalisierung in ungesehenen Leveln untersucht. Die Evaluation ist anhand vorab erstellter Fragen realisiert. Diese geschieht unter Verwendung zweier visueller Veränderungen des Agenten-Inputs, während des Trainings und/oder der Evaluation: farbliche Änderungen von Spielelementen wie bspw. der Orbs und visuell-semantische Invertierung von zwei unterschiedlichen Spielelementen. \\
Diese Experimente folgen drei Sets an Fragen, welche in Kapitel \ref{subsec_EXP_durch_fragen} ausführlich aufgelistet sind. 

In den Experimenten des Unterkapitels \ref{absch_EXP_durch_serie1} wird Data Augmentation in Form von farblicher Änderung oder optischer Maskierung mancher Spielelemente betrieben. Hiermit wird einerseits durch optische Maskierung untersucht, wie relevant gewisse Objekte des Environments, hier die kleinen Orbs, für einen erfolgreichen Abschluss einer Episode sind. Andererseits wird mittels farblicher Änderungen untersucht, ob während des Trainings eine Konzeptualisierung der kleinen Orbs stattfindet. Mit einem ausreichenden Konzept eines kleinen Orbs, sollten farbliche Änderungen des Orbs in der Evaluation zu keinem Verlust der Performance führen. Zudem wird untersucht, ob mehrere Level innerhalb einer kleinen Anzahl an Zeitschritten auswendig gelernt werden können, und wie relevant die visuelle Information der kleinen Orbs dabei ist.

Die Experimente in Unterkapitel \ref{absch_EXP_durch_serie2} untersuchen, wie ein trainierter Agent in der Evaluation abschneidet, wenn Spielelemente, wie die großen Orbs und die Geister, auf visuelle Weise semantisch invertiert sind. Hierbei wird untersucht, ob während der Feature Extraction auch nicht-optische Features, wie bspw. ein Bewegungsmuster, ein Indikator für ein Spielelement sein kann. Des Weiteren wird durch eine Invertierung der kleinen Orbs und der Mauern überprüft, ob ein Agent ein Grundverständnis des Spiels aufweist, oder ob er dadurch auf die Leistung eines Random-Agenten zurückfällt. 

Das erste erwähnte Set an Fragen \ref{subsec_EXP_durch_fragen} beschäftigt sich mit der Reproduktion der Ergebnisse der Arbeit \cite{cobbe2019procgen} und der generellen Auswirkung der prozeduralen Level-Generierung und schafft lediglich das Fundament der folgenden Experimente.\\



Kapitel \ref{hauptabschnitt_2} beschreibt die verwandten Arbeiten, welche für die vorliegende Arbeit als Inspiration dienen oder technisch relevant sind. Das dritte Kapitel (\ref{hauptabschnitt_3}) legt die theoretische Grundlage für die Arbeit dar. Hier wird unter anderem der PPO Algorithmus und die IMPALA-Architektur erläutert. Das Kapitel \ref{hauptabschnitt_4} befasst sich mit dem Setup der Experimente. Hier wird das Environment beschrieben und seine Eigenschaften erläutert. Kapitel \ref{hauptabschnitt_5} handelt von der Durchführung der Experimente. Wie zuvor erwähnt, sind die Experimente in Sets unterteilt. Diese Sets werden systematisch mit Experimenten untersucht. Das sechste Kapitel (\ref{ausblick}) gibt eine Ausblick über offene oder während der Bearbeitung des Themas aufgekommene Fragen und mögliche Abläufe für Experimente, um diese zu untersuchen. Kapitel \ref{konklusion} fasst die Arbeit abschließend zusammen und beschreibt die Erkenntnisse, welche aus den Experimenten gewonnen werden können. 


\newpage