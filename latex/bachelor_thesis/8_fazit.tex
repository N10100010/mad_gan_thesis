\section{Konklusion}\label{konklusion}

%- optische Abweichung von Trainings- zu Evaluation-Umgebung haben fatale Auswirkungen auf die Performance 
%	- Herleitung durch Orbs maskieren 
%		- visuell nicht vorhanden 
%		
%	- Überleitung zu Abstufungen grün 
%	- Hinleitung zu RGB 
%	- !!! Eine Veränderung der Pixel-Verteilung hat starke Auswirkungen auf die allgemeine Performance 

Ziel dieser Arbeit ist die Untersuchung der Generalisierung des eingesetzten IMPALA-Netzwerks \ref{absch_setup_impala} im Zusammenspiel mit dem verwendeten PPO Algorithmus \ref{absch_RL_ppo} in der Procgen-Umgebung \cite{cobbe2019procgen}. Dies geschieht anhand unterschiedlicher visueller Augmentationen des Trainings und/oder der Evaluation. Für die Untersuchung werden drei Sets an Fragen verwendet, welche sich jeweils mit unterschiedlichen Thematiken befassen. 

Das erste Set untersucht die Auswirkung der prozeduralen Level-Generierung auf die Generalisierung des Agenten. Hier wird vorab die verwendete Implementierung auf ihre Richtigkeit überprüft und mit den Ergebnissen des Procgen-Papers (\cite{cobbe2019procgen}) verglichen. Der Vergleich zeigte, dass die Implementierung im Stande ist die Ergebnisse des Papers für das Chaser-Environment zu reproduzieren. Das beantwortet die erste Frage des ersten Sets aus \ref{par_EXP_durch_fragen1} mit "Ja". Die Untersuchung zur Auswirkung der prozeduralen Level-Generierung vergleicht zwei Agenten, wobei einer mit einer beschränkten Anzahl (von 200 Leveln) und der andere mit einer unbeschränkten Anzahl an Leveln, beide für 80 Mio. Zeitschritte, trainiert werden. Der Agent mit einer fixen Anzahl an Trainings-Leveln generalisiert zwar ausreichend, um bei der Evaluation in unbekannten Leveln zu bestehen, jedoch weist die Evaluation eine Generalisierungslücke zwischen Training und Evaluation auf. Der Agent mit unbeschränkt vielen Trainings-Leveln ist in der Lage diese Lücke zu schließen. Die Auswirkungen der prozeduralen Generierung und der daraus resultierende, theoretisch endlose Level-Stream für den Agenten, sind somit eindeutig hilfreich, um in unbekannten Leveln besser abzuschneiden. 

Das zweite Set befasst sich mit Farblichen Änderungen der kleinen Orbs und dem Hintergrund des Spiels. Hierbei wird untersucht, welche Auswirkungen die visuelle Information der kleinen Orbs auf die Performance hat und wie robust die Generalisierung eines Agenten gegenüber Farbänderungen der kleinen Orbs ist. Weiter wird in diesem Set untersucht, wie viele Level die verwendete IMPALA-Architektur (\ref{absch_setup_impala}) auswendig lernen kann und welche Rolle die visuelle Orb-Information dabei spielt. Die Beantwortung der ersten beiden Punkte im zweiten Set zeigen auf, dass die Einstellung, welche die Agenten hervorbrachte und welche am besten generalisiert haben, keine Robustheit gegenüber farblichen Änderungen der kleinen Orbs aufweisen. Des Weiteren wird hier gezeigt, dass die visuelle Information des Orbs eine notwendige Hilfe bietet um in der Evaluation erfolgreich zu bestehen. Die Experimente zum Auswendiglernen der Level zeigen, dass die verwendete Architektur in der Lage ist, in 5 Mio. Zeitschritten 3 Level erfolgreich auswendig zu lernen. Maskiert man jedoch im Training und in der Evaluation die Orbs visuell, so kann lediglich ein Leveln gelernt werden. Das und der konstant niedrigere Reward der Agenten ohne visuelle Orb-Information aus Abbildung \ref{fig:grph_green_80Mio_inflvl_15act_Training_evalAsTraining_1to15}, unterstützt die Hypothese aus \ref{sub_absch_EXP_durch_serie1_orbsMaskieren}, dass die Orbs eine notwendige visuelle Information darstellen, um das Chaser-Environment erfolgreich abzuschließen. \\
Allgemein zeigen die Ergebnisse dieses Sets, dass die eingesetzte Implementierung stark auf Farben overfitted und wenig bis garnicht robust gegenüber farblichen Änderungen ist. Die Ergebnisse der Experimente aus Unterkapitel \ref{sub_absch_EXP_durch_farbÄnderungen} untermauern diese Aussage. 

Das dritte Set untersucht die Auswirkungen visueller semantischer Invertierungen, verschiedener Spielelemente während der Evaluation. Hierfür werden in einem Experiment die Sprites von Mauer und kleinem Orb vertauscht und in einem weiteren die Sprites des großen Orbs und der Geister. Die Invertierung von Mauer und kleinem Orb stellt dabei eine größere Veränderung der Pixel-Verteilung dar, als die Invertierung von großen Orbs und Geistern. Die prozedurale Level-Generierung bringt immer mehr kleine Orbs als Mauern auf das Spielfeld. Eine Invertierung dieser beiden Sprites vertauscht somit auch das Verhältnis, wie oft jedes Spielelemente auf dem Spielfeld vorkommt. Die Invertierung von großen Orbs und Geistern stellt im Gegensatz zu der anderen Invertierung keine Veränderung der Pixel-Verteilung dar, da Geister und große Orbs im Verhältnis 1:1 vorkommen. Von beiden gibt es jeweils drei. Jedoch zeigen die Ergebnisse der Experimente, dass die Einstellung, die die Agenten hervorbringt, welche die beste Generalisierung aufweisen, bei dieser Art der Invertierung versagen. Eine Videoanalyse der Evaluation beider Invertierung zeigt ein interessantes Verhalten. In beiden Experimenten hingen die Agenten die meiste Zeit einer Episode in einem Eck des Labyrinths und sind kaum von der Stelle weggekommen, an der sie in der Episode erscheinen sind. Das erklärt die Rewards ähnlich dem Random-Agent. Bei der Analyse der Evaluation wird jedoch ersichtlich, dass sich der Agent aus dem Experiment zur Invertierung von großen Orbs und Geistern nicht davor scheut einen großen Orb alias einen Geist, welcher sich mit dem Bewegungsmuster der Geister auf ihn zubewegt, entgegen zu gehen. Wohingegen ein Geist alias ein großer Orb, welcher stationär an einer Stelle ist, potentiell gemieden wird. \\
Zusammen mit den Ergebnissen des vorangegangenen Sets kann man davon ausgehen, dass die Feature-Extraction-Pipeline stark auf die Texturen der jeweiligen Spielelemente overfitted und eine Invertierung zweier Sprites, selbst wenn sie im Verhältnis 1:1 vorkommen, eine unüberwindbare Hürde darstellen. \newline

Wie in Kapitel \ref{hauptabschnitt_2} bereits erwähnt, besteht in RL generell das Problem des Overfittings und die daraus resultierende schlechte Generalisierung. Die prozedurale Generierung von Procgen schafft es im Chaser-Environment die Generalisierungslücke zu schließen \ref{subsec:absch_EXP_durch_reproduktion_generalisierung}. Der Agent mit unbeschränkter Level-Anzahl im Training zeigt zu seinen bekannten Trainingsbedingungen, dass nahezu jedes unbekannte Level beim ersten Versuch geschafft wird. Verändert man jedoch die Farbe eine Spielelements bspw. der Orbs, fällt die Performance auf die des Random-Agenten herab. Diese Generalisierung findet somit nahezu ausschließlich innerhalb der ihm bekannten Pixel-Verteilung des Trainings statt. Das stellt eine weitere Problematik in Simulatoren dar. Arbeiten wie \cite{raileanu2020automatic} bieten die Möglichkeit die Pixel-Verteilung im Training, durch verschiedene visuelle Augmentationen mit einer höheren Diversität zu versehen. Somit kann die Problematik der Generalisierung in Procgen von zwei unterschiedlichen Aspekten angegangen werden. Einerseits mit der prozeduralen Generierung, welche theoretisch unendlich viele Versionen des gleichen Levels bereitstellen kann. Andererseits würde mit einem Ansatz wie in \cite{raileanu2020automatic} oder \cite{zhang2018natural} die visuelle Diversität des Environments erhöht werden. Diese beiden Ansätze schaffen Grundlagen für systematische Generalisierung.


