\section{Conclusion}\label{conclusion}

This work assessed generative quality via Fréchet Inception Distance (FID) and Inception Score (IS), alongside generative data augmentation (GDA) utility through classifier F1 scores in data replacement and expansion settings (\ref{body_experiment_succession}). Several nuanced insights emerged. For Multi-Agent Diverse GAN (MADGAN), increasing the number of generators (\(K\)) generally improved generative quality, particularly FID. However, this did not consistently lead to better GDA performance. While often competitive, MADGAN-based GDA did not consistently outperform simpler baselines or traditional techniques (TDA), showing that improved FID alone is not a reliable indicator of augmentation effectiveness.

Introducing the conditional variant (cMADGAN) revealed a more complex relationship between \(K\) and task-specific utility. While FID and IS mostly peaked with fewer generators, the optimal \(K\) for GDA frequently diverged. Thus, \(K\) is a critical hyperparameter requiring careful tuning. Specific cMADGAN setups (e.g., \(K=10\) on MNIST, \(K=7\) on Fashion-MNIST) achieved F1 scores rivaling or surpassing TDA—even when not optimal in terms of FID and IS. This suggests augmentation benefits from multi-generator diversity not fully captured by standard metrics. Conversely, in replacement scenarios, cMADGAN showed substantial variability across generators, reducing reliability.

A recurring theme was the dataset- and task-specific nature of GDA effectiveness. No single generative method proved universally superior. TDA remained a strong and consistent baseline, but cMADGAN’s success in expansion tasks underscores the promise of more advanced GDA approaches. Optimizing \(K\) involves trade-offs between peak performance, consistency, and average utility.

Ultimately, GDA, like traditional augmentation techniques, is one of many tools to enhance classification performance. The best results may come from a combination of diverse strategies. With classification performance on a validation set in mind, the best-performing classifier may arise when trained on samples originating from both TDA- and GDA-based techniques. Regardless of the method, effectiveness must be evaluated using metrics aligned with specific task goals.

This study highlights the potential of multi-generator GANs, particularly cMADGAN, for advanced GDA. Effective use requires nuanced understanding of training dynamics and metric interpretation. Key findings — especially on the role of \(K\) and the disconnect between FID/IS and downstream utility — motivate further research described in Chapter \ref{chapter_remarks}, including deeper exploration of generator specialization. Such work will be vital to fully harnessing the potential of these architectures for generating diverse, effective synthetic data.


%Data generated by a GAN, may it be a cGAN or a MADGAN, may not fully capturethe distribution characteristics of its
%training data. Though, generated images do visually appear realistic, they may only partially reflect the
%statistical characteristics of the original data. This can lead to synthetic images that appear \textit{good} to a
%human inspector, but may contain amounts of noice that may interfere with a subsequent classifier.

%

%%% Goood answer:

%from an information theoratival standpoint, a generative model G trained on data X, distilling knowledge into a
%classifier C should not offer more information that what was already present in X.

%%% https://www.reddit.com/r/MachineLearning/comments/s88s72/comment/htfud7b/?utm_source=share&utm_medium=web3x&
%utm_name=web3xcss&utm_term=1&utm_content=share_button

%%% generally, the entire thread is an interesting critical POW against GANs for GDA in genral
