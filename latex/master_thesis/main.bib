% Last Modified: 2025-03-16
%       author
%       year


@misc{biswas2023generativeadversarialnetworksdata,
  title         = {Generative Adversarial Networks for Data Augmentation},
  author        = {Angona Biswas and MD Abdullah Al Nasim and Al Imran and Anika Tabassum Sejuty and Fabliha Fairooz and Sai Puppala and Sajedul Talukder},
  year          = {2023},
  eprint        = {2306.02019},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI},
  url           = {https://arxiv.org/abs/2306.02019}
}

@misc{ghosh2018madgan,
  title         = {Multi-Agent Diverse Generative Adversarial Networks},
  author        = {Arnab Ghosh and Viveka Kulharia and Vinay Namboodiri and Philip H. S. Torr and Puneet K. Dokania},
  year          = {2018},
  eprint        = {1704.02906},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1704.02906}
}

@misc{goodfellow2014generativeadversarialnetworks,
  title         = {Generative Adversarial Networks},
  author        = {Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
  year          = {2014},
  eprint        = {1406.2661},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
  url           = {https://arxiv.org/abs/1406.2661}
}

@misc{heusel2018ganstrainedtimescaleupdate,
  title         = {GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
  author        = {Martin Heusel and Hubert Ramsauer and Thomas Unterthiner and Bernhard Nessler and Sepp Hochreiter},
  year          = {2018},
  eprint        = {1706.08500},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1706.08500}
}

@misc{isola2018imagetoimagetranslationconditionaladversarial,
  title         = {Image-to-Image Translation with Conditional Adversarial Networks},
  author        = {Phillip Isola and Jun-Yan Zhu and Tinghui Zhou and Alexei A. Efros},
  year          = {2018},
  eprint        = {1611.07004},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1611.07004}
}

@misc{ji2020comprehensivesurveydeepmusic,
  title         = {A Comprehensive Survey on Deep Music Generation: Multi-level Representations, Algorithms, Evaluations, and Future Directions},
  author        = {Shulei Ji and Jing Luo and Xinyu Yang},
  year          = {2020},
  eprint        = {2011.06801},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SD},
  url           = {https://arxiv.org/abs/2011.06801}
}

@misc{Krizhevsky2009learning,
  author       = {Alex Krizhevsky},
  title        = {Learning Multiple Layers of Features from Tiny Images},
  howpublished = {Technical Report, University of Toronto},
  year         = {2009},
  url          = {https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf}
}

@article{lecun2010mnist,
  title   = {MNIST handwritten digit database},
  author  = {LeCun, Yann and Cortes, Corinna and Burges, CJ},
  journal = {ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},
  volume  = {2},
  year    = {2010}
}

@misc{ledig2017photorealisticsingleimagesuperresolution,
  title         = {Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network},
  author        = {Christian Ledig and Lucas Theis and Ferenc Huszar and Jose Caballero and Andrew Cunningham and Alejandro Acosta and Andrew Aitken and Alykhan Tejani and Johannes Totz and Zehan Wang and Wenzhe Shi},
  year          = {2017},
  eprint        = {1609.04802},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1609.04802}
}

@article{Li2025comprehensivesurvedeepimages,
  author  = {Jun Li, Chenyang Zhang, Wei Zhu and Yawei Ren},
  title   = {A Comprehensive Survey of Image Generation Models Based on Deep Learning},
  journal = {Annals of Data Science},
  volume  = {12},
  pages   = {141–170},
  year    = {2025},
  month   = {February},
  doi     = {10.1007/s40745-024-00544-1},
  url     = {https://doi.org/10.1007/s40745-024-00544-1}
}

@article{make5010019,
  author   = {Strelcenia, Emilija and Prakoonwit, Simant},
  title    = {A Survey on GAN Techniques for Data Augmentation to Address the Imbalanced Data Issues in Credit Card Fraud Detection},
  journal  = {Machine Learning and Knowledge Extraction},
  volume   = {5},
  year     = {2023},
  number   = {1},
  pages    = {304--329},
  url      = {https://www.mdpi.com/2504-4990/5/1/19},
  issn     = {2504-4990},
  abstract = {Data augmentation is an important procedure in deep learning. GAN-based data augmentation can be utilized in many domains. For instance, in the credit card fraud domain, the imbalanced dataset problem is a major one as the number of credit card fraud cases is in the minority compared to legal payments. On the other hand, generative techniques are considered effective ways to rebalance the imbalanced class issue, as these techniques balance both minority and majority classes before the training. In a more recent period, Generative Adversarial Networks (GANs) are considered one of the most popular data generative techniques as they are used in big data settings. This research aims to present a survey on data augmentation using various GAN variants in the credit card fraud detection domain. In this survey, we offer a comprehensive summary of several peer-reviewed research papers on GAN synthetic generation techniques for fraud detection in the financial sector. In addition, this survey includes various solutions proposed by different researchers to balance imbalanced classes. In the end, this work concludes by pointing out the limitations of the most recent research articles and future research issues, and proposes solutions to address these problems.},
  doi      = {10.3390/make5010019}
}

@mics{NIPS2012_c399862d,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
  volume    = {25},
  year      = {2012}
}

@misc{pathak2016contextencodersfeaturelearning,
  title         = {Context Encoders: Feature Learning by Inpainting},
  author        = {Deepak Pathak and Philipp Krahenbuhl and Jeff Donahue and Trevor Darrell and Alexei A. Efros},
  year          = {2016},
  eprint        = {1604.07379},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1604.07379}
}

@misc{perez2017effectivenessdataaugmentationimage,
  title         = {The Effectiveness of Data Augmentation in Image Classification using Deep Learning},
  author        = {Luis Perez and Jason Wang},
  year          = {2017},
  eprint        = {1712.04621},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1712.04621}
}

@misc{reed2016generativeadversarialtextimage,
  title         = {Generative Adversarial Text to Image Synthesis},
  author        = {Scott Reed and Zeynep Akata and Xinchen Yan and Lajanugen Logeswaran and Bernt Schiele and Honglak Lee},
  year          = {2016},
  archiveprefix = {arXiv},
  eprint        = {1605.05396},
  primaryclass  = {cs.NE},
  note          = {arXiv:1605.05396},
  url           = {https://arxiv.org/abs/1605.05396}
}

@misc{salimans2016improvedtechniquestraininggans,
  title         = {Improved Techniques for Training GANs},
  author        = {Tim Salimans and Ian Goodfellow and Wojciech Zaremba and Vicki Cheung and Alec Radford and Xi Chen},
  year          = {2016},
  eprint        = {1606.03498},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1606.03498}
}

@article{shorten2019survey,
  author  = {Connor Shorten and Taghi M. Khoshgoftaar},
  title   = {A survey on Image Data Augmentation for Deep Learning},
  journal = {Journal of Big Data},
  volume  = {6},
  number  = {1},
  pages   = {60},
  year    = {2019},
  month   = {July},
  doi     = {10.1186/s40537-019-0197-0},
  url     = {https://doi.org/10.1186/s40537-019-0197-0},
  issn    = {2196-1115}
}


@article{Tanno2022repairingneuralnetworkfromcorrupt,
  author   = {Tanno, Ryutaro and F.Pradier, Melanie and Nori, Aditya and Li, Yingzhen},
  year     = {2022},
  month    = {07},
  pages    = {19},
  title    = {Repairing Neural Networks by Leaving the Right Past Behind},
  doi      = {10.48550/arXiv.2207.04806},
  abstract = {Prediction failures of machine learning models often arise from deficiencies in trainingdata, such as incorrect labels, outliers, and selection biases. However, such data pointsthat are responsible for a given failure mode are generally not known a priori, let alone amechanism for repairing the failure. This work draws on the Bayesian view of continuallearning, and develops a generic framework for both, identifying training exampleswhich have given rise to the target failure, and fixing the model through erasinginformation about them. This framework naturally allows leveraging recent advancesin continual learning to this new problem of model repairment, while subsumingthe existing works on influence functions and data deletion as specific instances.Experimentally, the proposed approach outperforms the baselines for both identificationof detrimental training data and fixing model failures in a generalisable manner.}
}


@misc{wang2023multimodalityguidedimagestyletransfer,
  title         = {Multimodality-guided Image Style Transfer using Cross-modal GAN Inversion},
  author        = {Hanyu Wang and Pengxiang Wu and Kevin Dela Rosa and Chen Wang and Abhinav Shrivastava},
  year          = {2023},
  eprint        = {2312.01671},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2312.01671}
}

@mics{WanLiZeiler2013,
  title     = {Regularization of Neural Networks using DropConnect},
  author    = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and Le Cun, Yann and Fergus, Rob},
  booktitle = {Proceedings of the 30th International Conference on Machine Learning},
  pages     = {1058--1066},
  year      = {2013},
  editor    = {Dasgupta, Sanjoy and McAllester, David},
  volume    = {28},
  number    = {3},
  series    = {Proceedings of Machine Learning Research},
  address   = {Atlanta, Georgia, USA},
  month     = {17--19 Jun},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v28/wan13.pdf},
  url       = {https://proceedings.mlr.press/v28/wan13.html},
  abstract  = {We introduce DropConnect, a generalization of DropOut, for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recoginition benchmarks can be obtained by aggregating multiple DropConnect-trained models.}
}

@article{Wickramaratnecondganfordataaugmentation,
  author   = {Wickramaratne, Sajila D.  and Mahmud, Md.Shaad },
  title    = {Conditional-GAN Based Data Augmentation for Deep Learning Task Classifier Improvement Using fNIRS Data},
  journal  = {Frontiers in Big Data},
  volume   = {4},
  year     = {2021},
  url      = {https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2021.659146},
  doi      = {10.3389/fdata.2021.659146},
  issn     = {2624-909X},
  abstract = {<p>Functional near-infrared spectroscopy (fNIRS) is a neuroimaging technique used for mapping the functioning human cortex. fNIRS can be widely used in population studies due to the technology’s economic, non-invasive, and portable nature. fNIRS can be used for task classification, a crucial part of functioning with Brain-Computer Interfaces (BCIs). fNIRS data are multidimensional and complex, making them ideal for deep learning algorithms for classification. Deep Learning classifiers typically need a large amount of data to be appropriately trained without over-fitting. Generative networks can be used in such cases where a substantial amount of data is required. Still, the collection is complex due to various constraints. Conditional Generative Adversarial Networks (CGAN) can generate artificial samples of a specific category to improve the accuracy of the deep learning classifier when the sample size is insufficient. The proposed system uses a CGAN with a CNN classifier to enhance the accuracy through data augmentation. The system can determine whether the subject’s task is a Left Finger Tap, Right Finger Tap, or Foot Tap based on the fNIRS data patterns. The authors obtained a task classification accuracy of 96.67% for the CGAN-CNN combination.</p>}
}

@misc{xiao2017fashionmnist,
  title         = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  author        = {Han Xiao and Kashif Rasul and Roland Vollgraf},
  year          = {2017},
  eprint        = {1708.07747},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}


@article{Ying2019overfittinganditssolutions,
  doi       = {10.1088/1742-6596/1168/2/022022},
  url       = {https://dx.doi.org/10.1088/1742-6596/1168/2/022022},
  year      = {2019},
  month     = {feb},
  publisher = {IOP Publishing},
  volume    = {1168},
  number    = {2},
  pages     = {022022},
  author    = {Ying, Xue},
  title     = {An Overview of Overfitting and its Solutions},
  journal   = {Journal of Physics: Conference Series},
  abstract  = {Overfitting is a fundamental issue in supervised machine learning which prevents us from perfectly generalizing the models to well fit observed data on training data, as well as unseen data on testing set. Because of the presence of noise, the limited size of training set, and the complexity of classifiers, overfitting happens. This paper is going to talk about overfitting from the perspectives of causes and solutions. To reduce the effects of overfitting, various strategies are proposed to address to these causes: 1) “early-stopping” strategy is introduced to prevent overfitting by stopping training before the performance stops optimize; 2) “network-reduction” strategy is used to exclude the noises in training set; 3) “data-expansion” strategy is proposed for complicated models to fine-tune the hyper-parameters sets with a great amount of data; and 4) “regularization” strategy is proposed to guarantee models performance to a great extent while dealing with real world issues by feature-selection, and by distinguishing more useful and less useful features.}
}

@misc{dong2017museganmultitracksequentialgenerative,
      title={MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment},
      author={Hao-Wen Dong and Wen-Yi Hsiao and Li-Chia Yang and Yi-Hsuan Yang},
      year={2017},
      eprint={1709.06298},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/1709.06298},
}

@misc{li2022ttsgantransformerbasedtimeseriesgenerative,
      title={TTS-GAN: A Transformer-based Time-Series Generative Adversarial Network},
      author={Xiaomin Li and Vangelis Metsis and Huangyingrui Wang and Anne Hee Hiong Ngu},
      year={2022},
      eprint={2202.02691},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2202.02691},
}

@misc{yu2017seqgansequencegenerativeadversarial,
      title={SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient},
      author={Lantao Yu and Weinan Zhang and Jun Wang and Yong Yu},
      year={2017},
      eprint={1609.05473},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1609.05473},
}

@misc{xu2019modelingtabulardatausing,
      title={Modeling Tabular data using Conditional GAN},
      author={Lei Xu and Maria Skoularidou and Alfredo Cuesta-Infante and Kalyan Veeramachaneni},
      year={2019},
      eprint={1907.00503},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1907.00503},
}

@misc{mirza2014conditionalgenerativeadversarialnets,
      title={Conditional Generative Adversarial Nets},
      author={Mehdi Mirza and Simon Osindero},
      year={2014},
      eprint={1411.1784},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1411.1784},
}

@article{huang2022tutorial,
  author = {Huang, Y. and Fields, K. G. and Ma, Y.},
  title = {A Tutorial on Generative Adversarial Networks with Application to Classification of Imbalanced Data},
  journal = {Statistical Analysis and Data Mining},
  volume = {15},
  number = {5},
  pages = {543--552},
  year = {2022},
  doi = {10.1002/sam.11570}
}

@ARTICLE{LeCun1989firstcnnpaper,
  author={LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  journal={Neural Computation},
  title={Backpropagation Applied to Handwritten Zip Code Recognition},
  year={1989},
  volume={1},
  number={4},
  pages={541-551},
  keywords={},
  doi={10.1162/neco.1989.1.4.541}
}

@article{zhao2023gan,
  author={Zhao, Gaochang and Cai, Zhao and Wang, Xin and Dang, Xiaohu},
  title={GAN Data Augmentation Methods in Rock Classification},
  journal={Applied Sciences},
  volume={13},
  number={9},
  pages={5316},
  year={2023},
  doi={10.3390/app13095316}
}


@Article{cGANGDA2025asurveyreview,
AUTHOR = {Ribas, Lucas C. and Casaca, Wallace and Fares, Ricardo T.},
TITLE = {Conditional Generative Adversarial Networks and Deep Learning Data Augmentation: A Multi-Perspective Data-Driven Survey Across Multiple Application Fields and Classification Architectures},
JOURNAL = {AI},
VOLUME = {6},
YEAR = {2025},
NUMBER = {2},
ARTICLE-NUMBER = {32},
URL = {https://www.mdpi.com/2673-2688/6/2/32},
ISSN = {2673-2688},
ABSTRACT = {Effectively training deep learning models relies heavily on large datasets, as insufficient instances can hinder model generalization. A simple yet effective way to address this is by applying modern deep learning augmentation methods, as they synthesize new data matching the input distribution while preserving the semantic content. While these methods produce realistic samples, important issues persist concerning how well they generalize across different classification architectures and their overall impact in accuracy improvement. Furthermore, the relationship between dataset size and model accuracy, as well as the determination of an optimal augmentation level, remains an open question in the field. Aiming to address these challenges, in this paper, we investigate the effectiveness of eight data augmentation methods—StyleGAN3, DCGAN, SAGAN, RandAugment, Random Erasing, AutoAugment, TrivialAugment and AugMix—throughout several classification networks of varying depth: ResNet18, ConvNeXt-Nano, DenseNet121 and InceptionResNetV2. By comparing their performance on diverse datasets from leaf textures, medical imaging and remote sensing, we assess which methods offer superior accuracy and generalization capability in training models with no pre-trained weights. Our findings indicate that deep learning data augmentation is an effective tool for dealing with small datasets, achieving accuracy gains of up to 17%.},
DOI = {10.3390/ai6020032}
}

@article{wickramaratne2021conditional,
  author={Wickramaratne, S. D. and Mahmud, M. S.},
  title={Conditional-GAN Based Data Augmentation for Deep Learning Task Classifier Improvement Using fNIRS Data},
  journal={Frontiers in Big Data},
  volume={4},
  pages={659146},
  year={2021},
  doi={10.3389/fdata.2021.659146}
}

@article{jeong2022gan,
  author={Jeong, Jason and Patel, B. and Banerjee, I.},
  title={GAN augmentation for multiclass image classification using hemorrhage detection as a case-study},
  journal={Journal of Medical Imaging (Bellingham, Wash.)},
  volume={9},
  number={3},
  pages={035504},
  year={2022},
  doi={10.1117/1.JMI.9.3.035504}
}

@misc{durall2020combatingmodecollapsegan,
      title={Combating Mode Collapse in GAN training: An Empirical Analysis using Hessian Eigenvalues},
      author={Ricard Durall and Avraam Chatzimichailidis and Peter Labus and Janis Keuper},
      year={2020},
      eprint={2012.09673},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2012.09673},
}

@misc{ioffe2015batchnormalizationacceleratingdeep,
      title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
      author={Sergey Ioffe and Christian Szegedy},
      year={2015},
      eprint={1502.03167},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1502.03167},
}

@article{Krizhevsky2012traditionaldataaugmentation,
  author    = {Alex Krizhevsky and Ilya Sutskever and Geoffrey E. Hinton},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  journal   = {Communications of the ACM},
  volume    = {60},
  number    = {6},
  pages     = {84--90},
  year      = {2012},
  doi       = {10.1145/3065386}
}

@inproceedings{Simard2003bestpracticesforcnns,
  author    = {Patrice Y. Simard and Dave Steinkraus and John C. Platt},
  title     = {Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis},
  booktitle = {Seventh International Conference on Document Analysis and Recognition},
  pages     = {958--963},
  year      = {2003},
  doi       = {10.1109/ICDAR.2003.1227801}
}

@article{Nagy1966,
  author    = {George Nagy and Henry Shelton},
  title     = {Self-Corrective Character Recognition System},
  journal   = {IBM Journal of Research and Development},
  volume    = {11},
  number    = {6},
  pages     = {612--628},
  year      = {1967},
  doi       = {10.1147/rd.116.0612}
}


@INPROCEEDINGS{ImageNetDataset5206848,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition},
  title={ImageNet: A large-scale hierarchical image database},
  year={2009},
  volume={},
  number={},
  pages={248-255},
  keywords={Large-scale systems;Image databases;Explosions;Internet;Robustness;Information retrieval;Image retrieval;Multimedia databases;Ontologies;Spine},
  doi={10.1109/CVPR.2009.5206848}}

@article{VonNeumann1928Minimax,
  author    = {John von Neumann},
  title     = {Zur Theorie der Gesellschaftsspiele},
  journal   = {Mathematische Annalen},
  volume    = {100},
  number    = {1},
  pages     = {295--320},
  year      = {1928}
}
@misc{arjovsky2017wassersteingan,
      title={Wasserstein GAN},
      author={Martin Arjovsky and Soumith Chintala and Léon Bottou},
      year={2017},
      eprint={1701.07875},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1701.07875},
}

@misc{miyato2018spectralnormalizationgenerativeadversarial,
      title={Spectral Normalization for Generative Adversarial Networks},
      author={Takeru Miyato and Toshiki Kataoka and Masanori Koyama and Yuichi Yoshida},
      year={2018},
      eprint={1802.05957},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1802.05957},
}

@misc{mao2017squaresgenerativeadversarialnetworks,
      title={Least Squares Generative Adversarial Networks},
      author={Xudong Mao and Qing Li and Haoran Xie and Raymond Y. K. Lau and Zhen Wang and Stephen Paul Smolley},
      year={2017},
      eprint={1611.04076},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1611.04076},
}

@inproceedings{Odena201710.5555/3305890.3305954,
author = {Odena, Augustus and Olah, Christopher and Shlens, Jonathon},
title = {Conditional image synthesis with auxiliary classifier GANs},
year = {2017},
publisher = {JMLR.org},
abstract = {In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128 x 128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128 x 128 samples are more than twice as discriminable as artificially resized 32 x 32 samples. In addition, 84.7\% of the classes have samples exhibiting diversity comparable to real ImageNet data.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2642–2651},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{Radford2015DCGAN,
author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
year = {2016},
month = {11},
pages = {},
title = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}
}

@misc{Dumoulin2016TransposedConv,
      title={A guide to convolution arithmetic for deep learning},
      author={Vincent Dumoulin and Francesco Visin},
      year={2018},
      eprint={1603.07285},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1603.07285},
}

@article{Chang2024QDGenSampling,
  author    = {Chang, A. and Fontaine, M. C. and Booth, S. and Matarić, M. J. and Nikolaidis, S.},
  title     = {Quality-Diversity Generative Sampling for Learning with Synthetic Data},
  journal   = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume    = {38},
  number    = {18},
  pages     = {19805--19812},
  year      = {2024},
  doi       = {},
  publisher = {AAAI Press}
}

@article{Humayun2021MaGNET,
  author    = {Humayun, A. I. and Balestriero, R. and Baraniuk, R.},
  title     = {MaGNET: Uniform Sampling from Deep Generative Network Manifolds without Retraining},
  journal   = {arXiv preprint},
  volume    = {arXiv:2110.08009},
  year      = {2021},
  archivePrefix = {arXiv},
  eprint    = {2110.08009}
}

@inproceedings{Humayun2022PolaritySampling,
  author    = {Humayun, A. I. and Balestriero, R. and Baraniuk, R.},
  title     = {Polarity Sampling: Quality and Diversity Control of Pre-Trained Generative Networks via Singular Values},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages     = {10641--10650},
  year      = {2022}
}

@inproceedings{szegedy2016rethinking,
  title     = {Rethinking the Inception Architecture for Computer Vision},
  author    = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2016},
  pages     = {2818--2826}
}

@misc{Gavrikov2020VisualKeras,
  author = {Gavrikov, Paul},
  title = {visualkeras},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/paulgavrikov/visualkeras}},
}

@misc{fridman2019goodfellow,
  author       = {Fridman, Lex and Goodfellow, Ian},
  title        = {Ian Goodfellow: Generative Adversarial Networks (GANs) | Lex Fridman Podcast \#19},
  howpublished = {Audio podcast episode, \emph{Lex Fridman Podcast}, YouTube},
  year         = {2019},
  month        = {apr},
  day          = {18},
  url          = {https://www.youtube.com/watch?v=Z6rxFNMGdn0&t=3037s},
  note         = {Accessed: 2025-03-14}
}

@misc{Maucher2025,
  author       = {Maucher, Johannes},
  title        = {Markdown Convolutional Neural Network},
  howpublished = {\url{https://gitlab.mi.hdm-stuttgart.de/maucher/KI/-/blob/master/nb/N03ConvolutionNeuralNetworks.md}},
  note         = {Version: Mai 2025},
  year         = {2025},
  urldate      = {2025-05-15}
}

@article{Lu_Lu_2020,
   title={Dying ReLU and Initialization: Theory and Numerical Examples},
   volume={28},
   ISSN={1815-2406},
   url={http://dx.doi.org/10.4208/cicp.OA-2020-0165},
   DOI={10.4208/cicp.oa-2020-0165},
   number={5},
   journal={Communications in Computational Physics},
   publisher={Global Science Press},
   author={Lu Lu, Lu Lu and Yeonjong Shin, Yeonjong Shin and Yanhui Su, Yanhui Su and George Em Karniadakis, George Em Karniadakis},
   year={2020},
   month=jan, pages={1671–1706} }

@misc{karras2018progressivegrowinggansimproved,
      title={Progressive Growing of GANs for Improved Quality, Stability, and Variation}, 
      author={Tero Karras and Timo Aila and Samuli Laine and Jaakko Lehtinen},
      year={2018},
      eprint={1710.10196},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1710.10196}, 
}