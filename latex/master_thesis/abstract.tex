%\section*{Zusammenfassung}
%&Hier steht der Text, welcher den Inhalte der Arbeit zusammenfasst...


\section*{Abstract}
\pagestyle{empty}
Deep learning's reliance on extensive datasets underscores the critical need for effective data augmentation, particularly when data is scarce. While traditional data augmentation (TDA) is widely used, Generative Adversarial Networks (GANs) offer advanced capabilities for Generative Data Augmentation (GDA). This thesis investigates multi-generator GAN architectures—specifically the Multi-Agent Diverse GAN (MADGAN) and a novel conditional adaptation, cMADGAN—to address common GAN limitations like mode collapse and to evaluate their GDA efficacy. We systematically compared these models against TDA and standard GANs on MNIST and Fashion-MNIST, assessing GDA via downstream classifier F1 scores in data replacement and expansion scenarios, alongside FID and Inception Score for generative quality.

Our findings reveal nuanced impacts of generator count (K). While unconditional MADGAN's generative quality (FID) generally improved with K, its GDA utility was variable. The conditional cMADGAN, despite complex non-monotonic generative score trends with K, demonstrated compelling GDA capabilities. Notably, specific cMADGAN configurations achieved F1 scores in data expansion tasks that rivaled or even surpassed TDA, even when those particular K values were not optimal based on traditional FID/IS metrics.

This research highlights that multi-generator GANs, especially conditional variants like cMADGAN, hold significant potential for sophisticated GDA. However, their optimal configuration, particularly the number of generators, is highly dataset-dependent and task-specific. Crucially, GDA utility does not always directly correlate with standard generative model quality scores, emphasizing the need for careful, context-aware evaluation and tuning.

