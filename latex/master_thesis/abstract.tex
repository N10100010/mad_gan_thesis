\section*{Abstract}
\pagestyle{empty}
Deep learning's reliance on extensive datasets underscores the critical need for effective data augmentation, particularly when data is scarce. While traditional data augmentation (TDA) is widely used, Generative Adversarial Networks (GANs) offer advanced capabilities for Generative Data Augmentation (GDA). This thesis investigates multi-generator GAN architectures — specifically the Multi-Agent Diverse GAN (MADGAN) and a novel conditional adaptation, cMADGAN — to address common GAN limitations like mode collapse and to evaluate their GDA efficacy. This work systematically compares these models against TDA and standard GANs on MNIST and Fashion-MNIST, assessing GDA via downstream classifier F1 scores in data replacement and expansion scenarios, alongside Fréchet Inception Distance (FID) and Inception Score (IS) for generative quality. Further, the generator count (\(K\)) is taken into account and its impact on the downstream classifiers and FID and IS are analyzed. 

The findings reveal nuanced impacts of generator count (\(K\)). While unconditional MADGAN's generative quality (FID) generally improved with higher values for \(K\), its GDA utility was variable. The conditional cMADGAN, despite complex non-monotonic generative score trends with K, demonstrated compelling GDA capabilities. Notably, specific cMADGAN configurations achieved F1 scores in data expansion tasks that rivaled or even surpassed TDA, even when those particular values of \(K\) were not optimal based on traditional FID and IS metrics.

This research highlights that multi-generator GANs, especially conditional variants like cMADGAN, hold significant potential for sophisticated GDA. However, their optimal configuration, particularly the number of generators, is highly dataset-dependent and task-specific. Crucially, GDA utility does not always directly correlate with standard generative model quality scores, emphasizing the need for careful, context-aware evaluation and tuning.

