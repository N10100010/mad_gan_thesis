\section{Remarks} \label{chapter_remarks}
This section shall serve ideas that, with the implemented frameworks and trained models, could further be analyzed. These ideas have been omitted, to stick to the scope of this thesis.

\subsection{Connection between Generator Index and used Ratio to Classifier Performance}
\noindent \noindent With the current design of the study, especially for the multi-generator architectures, some granular insights into individual generator contributions to GDA performance are not fully elaborated in the main results. This is primarily due to the sheer volume of data from individual multi-generator architectures and the substantial number of corresponding classifiers trained. To illustrate the scale, the experimental workflow (as detailed in Section \ref{body_experiment_succession}) required evaluating both Replacement and Expansion GDA scenarios using the synthetic samples produced by each of the $K$ generators independently. Since each of these scenarios involved testing six distinct data compositions (e.g., varying ratios of real to synthetic data), a multi-generator model with $K$ components led to $K \times 6$ classifiers being trained for the Replacement experiments, and an equivalent number for the Expansion experiments\footnote{Recall that both Expansion and Replacement scenarios included six different ratios.}. Consequently, in cases with $K=10$ generators, figures depicting the results for just one scenario, such as Replacement, would ideally display $10 \text{ generators} \times 6 \text{ configurations} = 60$ distinct learning curves, leading to very dense visualizations (exemplified in Figures \ref{fig:app_strat_class_performance_expansion__val_f1__10__example_used_1} and \ref{fig:app_strat_class_performance_replacement__val_f1__10__example_used_1}). A significant amount of time has been invested, trying to add information about the generators' index to the graphs, visually connecting the generator creating the samples with the classifier using said data. With the Figures already containing such a high number of graphs, the decision was made to only reduce the opacity of the graphs, to indicate a trend via overlapping graphs, ultimately resulting in a darker visual representation. This is especially prominent in the second linked figure.

\subsection{Distinction of Modes}
\noindent A big advantage inherent to the MADGAN architecture is the encouraged diversity and enforced differentiation of modes, through the adjusted generator-identification objective of the discriminator \ref{theoretical_madgan_math}. Following those implications, if successful, the generators should focus on a specific subset of modes. In case of the MNIST dataset, this may result in N, potentially overlapping subsets of generator outputs\footnote{A mode is not necessarily associated with a specific class distribution.}.

This differentiation could manifest in various forms. The enforced distinction of modes for the generators might result in subtle differences that are not easily perceptible to the human eye. One way this could take form is that every generator learns a specific type of statistical signature or artifact, which one might loosely describe as a generator-specific signature or artifact resembling \textit{noise}. While a histogram analysis of binned pixel intensity ranges of the output images from respective generators could be a starting point to explore this, raw pixel values might be too coarse to capture meaningful distinctions that constitute a \textit{mode}. More robust approaches could involve analyzing histograms of features extracted by a pre-trained Convolutional Neural Network (CNN) or examining images in the frequency domain (e.g., via Fourier Transform) to identify generator-specific patterns in texture or composition.

A more precise and intuitive, form of mode differentiation is that specific generators may specialize in generating particular classes or styles within those classes. For instance, one generator might become adept at creating samples for the classes ([0, 3, 5, 8]), while another excels with ([1, 4, 7]). The existing approach of using a classification report for each generator-specific classifier\footnote{For this, the classification report from \textit{scikit-learn} was used. See: \url{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html}.} provides initial insight into which classes individual generators might favor. However, it's important to consider that modes may not perfectly align with predefined MNIST classes. A generator could specialize in a stylistic variation (e.g., thinly written digits vs. bold digits) that spans across or within classes. To delve deeper, analyzing the full confusion matrix for classifiers trained on each generator's output when tested against a balanced, real dataset can offer a more granular view of specialization. Reinforcing this, training a classifier on the output images of one generator and evaluating its performance on the outputs of other generators can effectively highlight how distinct their learned data distributions are. Poor cross-generator generalization would imply significant modal differences. Furthermore, employing unsupervised clustering techniques on the combined outputs of all generators before classification, and then examining the distribution of generator IDs within each discovered cluster, could reveal emergent modes independent of predefined class labels.

To further explore the notion of distinct modes, visualizing the outputs in a reduced-dimensional feature space can be highly illustrative. By taking image samples from each of the 10 generators and projecting them (or, more effectively, their feature vectors extracted by a pre-trained CNN) into a 2D or 3D space using techniques like t-SNE or UMAP, we can visually inspect the distributions. If generators have indeed learned distinct modes, their corresponding data points, when colored by generator ID, should form relatively separate clusters in this embedded space. Significant overlap between clusters would suggest less distinct or shared modes.

Complementary to visual inspection, quantifying the differences between generator outputs is crucial. This can be achieved by comparing intra-generator versus inter-generator image similarity or distance. The hypothesis is that if generators are pushed to distinct modes, images produced by the same generator should, on average, be more similar to each other than to images produced by different generators. While pixel-wise metrics (like MSE) can be used, feature-based distances (e.g., cosine distance between CNN-extracted features) or, ideally, perceptual similarity metrics such as LPIPS (Learned Perceptual Image Patch Similarity) or SSIM (Structural Similarity Index Measure) are more robust as they align better with human judgment of visual similarity. By calculating the average intra-generator similarity for each generator and the average inter-generator similarity for each pair of distinct generators, a clear pattern should emerge if modes are well-separated: higher similarity within a generator's output and lower similarity between outputs of different generators.

\newpage
