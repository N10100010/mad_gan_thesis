\section{Experiments Setup}\label{body_experiments_setup}

\subsection{Preliminary Remarks}\label{body_prelim}
Before presenting the specific experiments and corresponding results in Chapter \ref{body_experiments_results}, this section outlines essential preliminary remarks. These remarks cover common configurations, definitions, and methodological aspects that apply across the subsequent experimental evaluations, providing necessary context and avoiding repetitions later.

\subsubsection{Scope Limitation Regarding Standard CIFAR-10}
\label{setup_cifar10_scope}

The CIFAR-10 dataset, with its 32x32 pixel color images across 10 classes, represents a significant step-up  in complexity compared to MNIST or Fashion-MNIST and is a common benchmark in generative modeling. Consequently, initial plans involved evaluating the performance of multi-generator GAN approaches, including the original MADGAN \cite{ghosh2018madgan} and the adapted cMADGAN (Section \ref{theoretical_cmadgan}), on this standard dataset.

However, preliminary investigations encountered substantial difficulties in achieving stable training and generating samples of sufficient quality and diversity using these frameworks directly on standard CIFAR-10. Extensive efforts were undertaken to address these challenges, spanning a range of common techniques and modifications found in GAN literature. These included, but were not limited to:

\begin{itemize}
    \item Testing multiple generator and discriminator architectures with varying depths and capacities.
    \item Experimenting with different normalization layers (e.g., Batch Normalization, Spectral Normalization).
    \item Adjusting hyperparameters related to the Adam optimizer, particularly the learning rates for the generator(s) and discriminator, including various decay schedules and relative magnitudes.
    \item Implementing techniques designed to combat mode collapse and improve sample diversity, such as Mini-batch Discrimination.
    \item Tuning framework-specific hyperparameters like the latent dimension size and the importance of diversity (\(\lambdadiv\)) for cMADGAN.
    \item Employing standard stabilization methods like label smoothing.
\end{itemize}

Despite these comprehensive attempts, persistent issues such as training instabilities (e.g., oscillating losses, vanishing or exploding gradients) or consistently poor quantitative results (e.g., low IS, high FID relative to benchmarks or simpler datasets) indicated that the models did not converge to a satisfactory performance level on the standard CIFAR-10 dataset within the practical constraints of this study.

Given that the primary focus of this thesis is to investigate the comparative effects of different GAN-based data augmentation strategies (including MADGAN and cMADGAN), a pragmatic decision was made to exclude the standard CIFAR-10 dataset from the main comparative experiments presented in the subsequent results chapters. This allows the analysis to focus on datasets (MNIST, Fashion-MNIST) where the generative models achieved more stable and interpretable performance, enabling a clearer evaluation of the core research questions related to data augmentation effectiveness. The challenges encountered with standard CIFAR-10, while informative about the limitations of these specific multi-generator approaches on more complex data, are not subjected to further detailed analysis herein.

\subsubsection{Used Datasets}

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|p{7.5cm}|}
        \hline
        Dataset & N-Samples & Image Size & Description \\ \hline
        MNIST & 70,000  & $28 \times 28 \times 1$ & Grayscale images of handwritten digits (0â€“9). A classic benchmark for basic image classification tasks.\\ \hline
        Fashion-MNIST & 70,000  & $28 \times 28 \times 1$ & Grayscale images of clothing items. Provides more complexity and variability than MNIST, better reflecting real-world tasks.\\ \hline
    \end{tabular}
    \caption{Description of the used datasets for benchmarking.}
    \label{exp_setup_used_datasets}
\end{table}



\subsubsection{GAN: Architecture, Training and Data Augmentation}

\noindent\textbf{Architecture of the GANs}
In all experiments involving GAN models and their derivatives, the same network architecture has been employed for the \textit{MNIST} and \textit{Fashion MNIST} datasets. The architectures for the generators and their corresponding discriminator can be seen in the appendix (\ref{appendix_generator_architectures}, \ref{appendix_discriminator_architectures}). The used architecture is mentioned in the beginning of the respective experiment.

\noindent\textbf{Training}
For training all GAN-based models, including DCGAN, cGAN, MADGAN and cMADGAN, the \textit{Adam} optimizer has been utilized. The learning rate follows an exponentially decaying schedule throughout the training process.\\

\noindent\textbf{Data Augmentation}\label{body_experiment_dataaugmentation}
To increase the diversity of the training data for both generator and discriminator models, several traditional augmentation techniques have been applied. These include horizontal flips, brightness and contrast adjustments, and the addition of Gaussian noise.
Horizontal flips are applied with a probability of \(50\%\), except for the \textit{MNIST} dataset, where flips are omitted due to the semantic relevance of digit orientation. Brightness and contrast adjustments are always applied within a uniformly distributed range of \([-0.1, 0.1]\). The augmentation process adds Gaussian noise, by sampling from a normal distribution with mean \(0\) and standard deviation \(0.05\). Finally, the augmented images are clipped to the valid value range of \([-1, 1]\).

\subsubsection{Stratified Classifiers as measure for augmentation Quality}
To definitively evaluate the quality of the GDA, the fake images are used to replace and expand the underlying original datasets and train classification models on them. For this, the training datasets are specifically created to contain different ratios of real to fake images. To avoid biasing one class in the datasets over another, the datasets are stratified with respect to the number of samples per class. \footnote{Out of the used datasets, only the MNIST dataset is not originally stratified.} 

\subsubsection{Labeling unconditioned data}\label{body_experiment_labeling_data}
Due to the fact, that multiple experiments employ unconditioned GANs (\ref{exp_results_research_questions}), many images have been created with no corresponding label to them. To classify the unlabeled data, simple CNN classifiers, with adequate TDA, were utilized. The applied augmentations techniques are as follows: horizontal- and vertical shift by 0.1 relative to the absolute size of the image, rotation of up to 15 degree and horizontal flipping. As afore mentioned, flipping images along the horizontal axes is omitted for the MNIST dataset, due to semantical invalidity. Graphical depictions of the classifier used can be found in the appendix (\ref{appendix_classifiers}). The auxiliary classifiers were optimized for their weighted sum of accuracy and precision on the held-out test set of the respective dataset.\\


\subsubsection{Utilization of InceptionV3 for FID and IS}\label{body_experiment_inception_model}
It is crucial to note that the significant differences between the ImageNet domain (high-resolution, color, 1000 object classes) and datasets commonly used in GAN research like MNIST, Fashion-MNIST (low-resolution, grayscale), or CIFAR-10 (low-resolution, color, 10 simpler classes) represent a substantial domain gap \ref{theoretical_inception_model_limitaitions}. This gap may limit the effectiveness or absolute interpretability of InceptionV3-based scores for these specific datasets. Furthermore, due to the sensitivity of these scores to implementation details (e.g., image resizing methods, specific InceptionV3 weight versions), a direct comparison of scores calculated here to those from external literature is generally unreliable unless the evaluation methodology is verified to be identical. Therefore, within this thesis, IS and FID scores are primarily utilized for relative comparisons between the different models and experiments conducted herein, rather than for absolute benchmarking against potentially disparate external results. This context warrants careful consideration when analyzing the experimental outcomes presented later.


\subsection{Experimental Workflow}\label{body_experiment_succession}

The evaluation of each generative model adhered to a consistent experimental workflow, outlined below:

\begin{enumerate}
    \item \textbf{GAN Training:} The specific generative model (e.g., DCGAN, cGAN, MADGAN, cMADGAN) was trained on the target dataset. Model performance during training was monitored using predefined metrics such as FID and IS.

    \item \textbf{Synthetic Sample Generation (Per Generator):} After training, the individual generators \(G_i\) / \(G_{i, j}\) within the trained model (where \(i=1\) for single-generator models like DCGAN/cGAN, and \(i=1...K, j=0...(K-1)\) for multi-generator models MADGAN/cMADGAN with \(K \in \{3, 5, 7, 10\}\)) are used to create a distinct set of synthetic images, denoted as \(S_{\text{fake}, i}\). For each class in the original dataset, at least $6 000$ images are generated by each generator \(G_i\), resulting in \(K\) separate datasets of synthetic samples for each trained multi-generator model.
        \begin{itemize}
            \item \textit{Labeling Unconditional Samples:} For samples generated by unconditional models or generators (DCGAN, MADGAN generators), class labels are assigned to the images within each respective set \(S_{\text{fake}, i}\) using the pre-trained classifiers detailed in Section \ref{body_experiment_labeling_data}.
        \end{itemize}

    \item \textbf{Downstream Classifier Training (GDA Evaluation - Per Generator):} The effectiveness of Generative Data Augmentation (GDA) was evaluated separately for each generator \(G_i\) / \(G_{i, j}\) of a trained GAN-based model, using its corresponding synthetic sample set \(S_{\text{fake}, i}\) / \(S_{\text{fake}, i, j}\). For single-generator models, this step was performed once using \(S_{\text{fake}, 1}\). Using a fixed classifier architecture specific to each dataset (\ref{appendix_classifiers}) trained for $50$ epochs. The classifiers are trained under two distinct augmentation scenarios for each sample set \(S_{\text{fake}, i}\):
        \begin{itemize} \label{exp_setup_difference_replace_expand}
            \item \textit{Replacement Scenario:} This assessed the utility of synthetic data from generator \(G_i\) as a substitute for real data. Training commenced with a baseline classifier using $5 000$ real images per class. In subsequent steps, the number of real images per class was decreased by $1 000$ while the number of synthetic images per class (drawn from \(S_{\text{fake}, i}\)) was increased by $1 000$, maintaining a constant dataset size of $5 000$ images per class. This process continued until the final classifier was trained solely on $5 000$ synthetic images per class from \(S_{\text{fake}, i}\).
            \item \textit{Expansion Scenario:} This evaluated synthetic data from generator \(G_i\) as a supplement to real data. Training started with the same baseline ($5 000$ real images per class). The real dataset was then augmented by adding synthetic images from \(S_{\text{fake}, i}\) in increments of $1 000$ per class per step, reaching a maximum of $5 000$ synthetic images per class. The final classifier in this scenario was trained on a combined dataset of $5 000$ real and $5 000$ synthetic images per class from \(S_{\text{fake}, i}\).
        \end{itemize}
      Note that for each trained MADGAN or cMADGAN model with K generators, the full set of replacement and expansion classifier trainings were performed K times, once for each generator's synthetic dataset. The scenarios therefore result in a total of $K * 6$ classifiers for both of the scenarios. That is, five experiments altering the ratios of the respective experiment and a corresponding baseline. 

    \item \textbf{Downstream Classifier Evaluation:} The performance of all trained classifiers (baseline and those from the replacement and expansion scenarios for each generator set \(S_{\text{fake}, i}\)) was evaluated using predefined classification metrics \ref{theory_classification_performanc_metrics}. Results for multi-generator models may be presented as averages across the K generators or by selecting representative examples of specific generators with specified ratios between real and fake images.
\end{enumerate}

\noindent In total, this setup results in \( 2 \times 4 \times 2 \times 4 = 64 \) distinct experiment configurations, where:
\begin{itemize} \label{list_number_of_classifiers}
    \item 2 datasets (MNIST, Fashion-MNIST),
    \item 4 augmentation types,
    \item 2 experiment setups (expansion and replacement),
    \item and 4 different values for the number of generators trained \((k \in \{3, 5, 7, 10\})\).
\end{itemize}
Taking into account the varying ratios of real to fake images in the expansion/replacement scenarios, a total of \(1248\) separate classifiers were trained to evaluate the potential of the multi-agent architecture for GDA, including the experiments with cGAN and DCGAN architectures.

\subsection{Comparison of Classifier Performance}
Due to the fact that the MADGAN and cMADGAN architectures apply a multi-agent strategy i.e., training multiple generators for one model, only the best run is selected for direct comparison to others. For example, when comparing MADGAN to cMADGAN in \ref{exp_results_ans_q4}, the best performing subset of the respective GAN architecture is used for comparison. The rest of the experiments not discussed explicitly or only mentioned can be seen in the appendix \ref{app_strat_class_performance}.

\subsection{Hardware and Software Environment}

\subsubsection{Hardware}
All models trained in the context of this thesis were trained utilizing the \textit{Deeplearning Cluster} provided by the \textit{Hochschule der Medien - Stuttgart}. The cluster provides 8 nodes that with dedicated graphics cards supporting the \textit{CUDA} framework. The specific machines deployed in the cluster can be seen here \href{https://deeplearn.pages.mi.hdm-stuttgart.de/docs/}{Deeplearn cluster Documentation}\footnote{Link to the documentation for print versions: \url{https://deeplearn.pages.mi.hdm-stuttgart.de/docs/}.}.

\subsubsection{Software}
The code for the experiments was developed with the programming language python, using version \(3.9\). Models are based on the tensorflow ecosystem. The exact packages and their respective version can be found here \href{https://github.com/N10100010/mad_gan_thesis/blob/main/code/server_env.yml}{Aanconda environment}\footnote{Link to the environment file for print versions: \url{https://github.com/N10100010/mad_gan_thesis/blob/main/code/server_env.yml}.}.



\newpage
