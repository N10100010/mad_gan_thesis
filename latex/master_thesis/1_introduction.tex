\section{Introduction and Motivation}\label{introduction_and_motivation}
\pagestyle{fancy}
%\setcounter{page}{1}
\textit{Generative Adversarial Networks} (GANs) \cite{goodfellow2014generativeadversarialnetworks} and their variants revolutionized the field of computer vision in the year of 2014, enabling advancements in multiple areas of generating data. From \textit{Text to Image Synthesis} \cite{reed2016generativeadversarialtextimage}, \textit{Image Translation} \cite{isola2018imagetoimagetranslationconditionaladversarial}, \textit{Super Resolution} \cite{ledig2017photorealisticsingleimagesuperresolution}, \textit{Image Inpainting} \cite{pathak2016contextencodersfeaturelearning}, \textit{Style Transfer} \cite{wang2023multimodalityguidedimagestyletransfer} to \textit{Data Augmentation} \cite{shorten2019survey}, GANs have been used in a variety of applications.

% TODO: MORE REFERENCES HERE? - see mad gan main paper )(introduction) for more
The idea of using GANs for \textit{Generative Data Augmentation} (GDA) has already been applied successfully, e.g.: in computer vision \cite{Li2025comprehensivesurvedeepimages}, \cite{biswas2023generativeadversarialnetworksdata} or for creating music \cite{ji2020comprehensivesurveydeepmusic}. Especially the former survey \textit{A Comprehensive Survey of Image Generation Models Based on Deep Learning} has, along \textit{Variational Auto Encoders} (VAEs), a dedicated focus on GANs. Despite these achievements, in practice, GANs suffer from several challenges, complicating the training and inference process\footnote{A more detailed depiction of the problems during training of GANs are depicted in \ref{theory_gan_problems}}:

\begin{itemize}\label{problems_of_gans}
    \setlength{\itemsep}{-5pt}
    \item Mode Collapse
    \item Lack of inter-class Diversity
    \item Failure to Converge
    \item Vanishing Gradients \& Unstable Gradients
    \item Imbalance between Generator- and Discriminator Model
\end{itemize}

This thesis investigates the potential of using GANs --- specifically \textit{Multi-Agent Diverse Generative Adversarial Networks} (MADGANs) \cite{ghosh2018madgan} --- for Generative Data Augmentation. MADGANs aim to aid the first two of the afore mentioned in particular: Mode Collapse and Loss of inter-class Diversity. They, along other modifications, \textit{propose to modify the objective function of the discriminator, in which, along with finding the real and the fake samples, the discriminator also has to correctly predict the generator that generated the given fake sample.} \cite{ghosh2018madgan}. The goal of this adjustment of the discriminator is, that the discriminator has to push the generators towards distinct identifiable modes. While various strategies have been proposed to address mode collapse and inter-class diversity, MADGANs explicitly enforce mode separation by the introduction of multiple generators and the adjusted discriminator objective. This makes them particularly promising for GDA, as diverse samples and clear distinction of modes is crucial for training robust classifiers. In their paper, they experimentally show, that their architectural adjustment of GANs is generally capable of providing assistance for the first two of the mentioned problems.

The experiments in this work are structured into three major parts.

\paragraph{Set 1: Training and Analysis of GANs}  \label{thesis_goal_1}
The first set trains multiple variations of GANs, explicitly \textit{Deep Convolutional GANs} (DCGANs), \textit{Conditional GANs} (cGANs) and the afore introduced MADGANs, in addition to an adapted conditionalized version called \textit{Conditional Multi-Agent Diverse GANs} (cMADGANs) \ref{theoretical_cmadgan}.

\paragraph{Set 2: Generating and Classifying Unlabeled Images}  \label{thesis_goal_2}
The second set uses the afore trained generative models to create images. Specifically, the goal for this stage is to generate at least $6 000$ images per class in the respective datasets. Images without labels, originating from MADGANs and DCGANs, will be classified using auxiliary classifiers trained with traditional data augmentation techniques. When all samples are classified, the quality of the resulting images is scored by the \textit{Fréchet Inception Distance} (FID) \cite{heusel2018ganstrainedtimescaleupdate} and the \textit{Inception Score} (IS) \cite{salimans2016improvedtechniquestraininggans}.

\paragraph{Set 3: Training and Evaluating Classifiers}  \label{thesis_goal_3}
The third and most important set of experiments trains classifiers using the generated and labeled samples. For this, stratified classifiers with different ratios of real to fake images are trained and evaluated on the respective validation set. These experiments are split into two scenarios: Replacement- and Expansion Scenarios \ref{body_experiment_succession}. Their classification performance will be assessed using the \textit{F1 Score}.  \\

\noindent All of the above described is executed on the well-known benchmark datasets:
\begin{itemize}\label{used_datasets}
    \setlength{\itemsep}{-5pt}
    \item MNIST \cite{lecun2010mnist}
    \item Fashion MNIST \cite{xiao2017fashionmnist}
    %\item CIFAR10 \cite{Krizhevsky2009learning}
\end{itemize}


\paragraph{Aim of the Thesis}\label{aim_of_the_thesis}

The goal of this thesis is to explore how effective Multi-Agent GAN architectures, like MADGAN and a novel conditional adaptation developed herein, termed cMADGAN, can be used for Generative Data Augmentation (GDA). Specifically, it sets out to:

\begin{enumerate}
    \item Compare the generative quality of MADGAN and cMADGAN --- measured by the aforementioned Fréchet Inception Distance (FID) and Inception Score (IS) --- with more basic GAN variants like DCGANs and cGANs.
    \item Evaluate the effectiveness of the multi-generator models in practice by analyzing their impact on classifier performance, particularly F1 scores on the validation sets, within two distinct GDA scenarios: replacing portions of the training set with synthetic images, and expanding the training set by adding synthetic images.
    \item Analyze how MADGAN and cMADGAN perform when their GDA results are compared against those from both traditional data augmentation (TDA) and established GAN baselines like DCGAN and cGAN, to delineate their respective advantages and limitations.
\end{enumerate}
\noindent Through these investigations, this thesis aims to provide insights into the challenges and opportunities associated with employing multi-generator GANs for augmenting datasets such as MNIST and Fashion-MNIST. The exact research questions can be found in Chapter \ref{exp_results_research_questions}.

\newpage

Chapter two \ref{related_work} describes the relevant connected works of researchers relevant to this thesis. This chapter reviews how other research conducted experiments in the context of data augmentation, may it be generative or traditional.

The third chapter \ref{body_theoretical_background} gives an introduction to the theoretical background for the architectures and differences between them. First, a brief overview for classification models and metrics to judge their performance is given. This is followed by the background for data augmentation and different forms augmentations can enhance training data (\ref{theoretical_classification}, \ref{theoretical_tda}). Next, Generative Adversarial Networks and the specific GAN variants utilized in this thesis are presented. Starting with their vanilla version, and problems they can face during training, followed by the adaptations experimented with (\ref{theoretical_gan}, \ref{theoretical_dcgan}, \ref{theoretical_cgan}, \ref{theoretical_madgan}, \ref{theoretical_cmadgan}). This is followed by the insights on how the image scoring works, applied here.

Chapter four, \textit{Experiments Setup} presents preliminary remarks essential for understanding the experimental work. It outlines scope limitations defined for this thesis (\ref{body_experiments_setup}) and details the consistent experimental workflow applied (\ref{body_experiment_succession}).

The experimental results are presented in chapter five (\ref{body_experiments_results}). Here, the research questions are listed (\ref{exp_results_research_questions}) and answered, one after another \ref{exp_results_questions_answers}.

The closing three chapters are general remarks (\ref{chapter_remarks}), the outlook (\ref{outlook}) and the conclusion. The remarks mention further experiments, that can be conducted, given trained models and generated images, thereby raising interesting questions that remain unanswered in this work. In the Outlook (\ref{outlook}), further directions for research in the context of generative data augmentations are mentioned. Ultimately, the conclusion (\ref{conclusion}) closes the thesis, summarizing the experiments.

\newpage
