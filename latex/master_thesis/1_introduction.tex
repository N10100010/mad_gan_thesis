\section{Introduction and Motivation}\label{introduction_and_motivation}
\pagestyle{fancy}
%\setcounter{page}{1}
Generative Adversarial Netwrorks (GANs) \cite{goodfellow2014generativeadversarialnetworks} and their variants revolutionized the field of computer vision in the year of 2014, enabling advacements in multiple areas of generating data. From \textit{Text to Image Synthesis} \cite{reed2016generativeadversarialtextimage}, \textit{Image Translation} \cite{isola2018imagetoimagetranslationconditionaladversarial}, \textit{Super Resolution} \cite{ledig2017photorealisticsingleimagesuperresolution}, \textit{Image Inpainting} \cite{pathak2016contextencodersfeaturelearning}, \textit{Style Transfer} \cite{wang2023multimodalityguidedimagestyletransfer} to \textit{Data Augmentation} \cite{shorten2019survey}, GANs have been used in a variety of applications.

% TODO: MORE REFERENCES HERE? - see mad gan main paper )(introduction) for more
The idea of using GANs for Generative Data Augmentation (GDA) has already been applied sucessfully, e.g.: in computer vision \cite{Li2025comprehensivesurvedeepimages}, \cite{biswas2023generativeadversarialnetworksdata} or for creating music \cite{ji2020comprehensivesurveydeepmusic}. Especially the former survey \textit{A Comprehensive Survey of Image Generation Models Based on Deep Learning} has, along Variational Auto Encoders (VAEs), a dedicated focus on GANs. Despite these achievements, in practice, GANs suffer from several challenges, complicating the training and inference process:

\begin{itemize}\label{problems_of_gans}
    \setlength{\itemsep}{-5pt}
    \item Mode Collapse
    \item Loss of inter-class Diversity
    \item Failure to Converge
    \item Vanishing Gradiants \& Unstable Gradiants
    \item Imbalance between Generator- and Discriminator Model
\end{itemize}


This thesis investigates the potential of using GANs - specifically \textit{Multi-Agent Diverse Generative Adversarial Networks} (MADGANs) \cite{ghosh2018multiagentdiversegenerativeadversarial} - for Generative Data Augmentation. MADGANs aim to aid the first two of the afore mentioned in particular: Mode Collapse and Loss of inter-class Diversity. They, along other modifications, "\textit{propose to modify the objective function of the discriminator, in which, along with finding the real and the fake samples, the discriminator also has to correctly predict the generator that generated the given fake sample.}" \cite{ghosh2018multiagentdiversegenerativeadversarial}. The goal of this adjustment of the discriminator is, that the discriminator has to push the generators towards distinct identifiable modes. While various statigies have been proposed to adresse mode collapse and inter-class diversity MADGANs explicitly enfore mode seperation by introduction of multiple generators and the adjusted discriminator objective. This makes them particularly promising for GDA, as diverse samples and clear distinction of modes is crucial for training robust classifiers. In their paper, they experimentally show, that their architectural adjustment of GANs is generally capable of giving providing assistance for first two of the mentioned problems.

The experiments in this work are structured in to three major parts.
The first set trains and analyses GANs, explicitly MADGANs and Conditional GANs (CGANs). Here, the quality of the resulting images during training will be scored by the Fr√©chet Inception Distance (FID) \cite{heusel2018ganstrainedtimescaleupdate} and the Inception Score (IS) \cite{salimans2016improvedtechniquestraininggans}.
The secoond set uses the afore trained generative models to create images. Images without labels - images originating from MADGANs - will be classified using auxilary classifiers trained using classical data augmentation techniques.
The third and most significant set of experiments trains classifiers, using the generated data. For this, stratified classifiers, with differing numbers of real and fake images are trained and evaluated on the respective validation set and their classification performance will be assesed using standard metrics.
\footnote{The set of metrics used to asses the quality of the resulting clasifiers is defined in chapter Experiments Setup \ref{body_experiments_setup}.}
% TODO: define standard metrics? maybe?


All of the above described is executed on the following datasets:
\begin{itemize}\label{used_datasets}
    \setlength{\itemsep}{-5pt}
    \item MNIST \cite{lecun2010mnist}
    \item Fashion MNIST \cite{xiao2017fashionmnist}
    \item CIFAR10 \cite{Krizhevsky2009learning}
\end{itemize}


\paragraph{Aim of the Thesis}\label{aim_of_the_thesis}
The aim of the thesis is to investigate the potential use of MADGANs for GDA and compare their performance against traditional data augmentation techniques. Traditional techniques involve altering operation on the training images, such as flipping, rotating, cropping images, altering their contrasts and adding a small amount of noise to them.


\newpage
