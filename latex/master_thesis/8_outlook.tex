\section{Outlook}\label{ausblick}

Unstructured thoughts, hidden for review. 

% develop a resiliant system for mad gan to generate data
% create a meta study, to compare the performance of different gan architectures


% repair networks

% \cite{Tanno2022repairingneuralnetworkfromcorrupt}

% 

% 

% Measure the resulting diversity between the generatoed samples using the MS-SSIM scores \cite{Odena201710.5555/
% 3305890.3305954}; they did that as well

% 

% check the diversity of modes in the generated images. following the experiments shown figure 5 

% 

% \subsubsection{Weight Sharing across Generators in MADGAN}

% Is Weight Sharing Mandatory in MADGAN?

% 

% No, weight sharing is not strictly mandatory to implement the general idea of having multiple generators aiming for 
% diversity and fooling a single discriminator. You could implement K completely independent generator networks.

% 

% However, the weight-sharing strategy proposed by Ghosh et al. is a key architectural feature and contribution of 
% their specific MAD-GAN formulation. In their paper, they propose sharing the parameters of the initial, deeper 
% layers across all generators and only having separate (unshared) parameters for the final few layers of each 
% generator.

% 

% So, while not theoretically mandatory for any multi-generator setup, it's central to the design and efficiency 
% benefits highlighted in the original MAD-GAN paper. Deviating significantly from it means you are implementing a 
% different variant of a multi-generator diverse GAN.

% 

% Advantages of Weight Sharing (as proposed in MAD-GAN):

% 

% Parameter Efficiency: This is a major advantage. Instead of storing and training K full generator networks, you 
% store one large shared network base and K smaller "heads" (the final unshared layers). This drastically reduces the 
% total number of parameters.

% Lower Memory Footprint: Requires less GPU memory, making it feasible to train more generators or use larger base 
% networks.

% Faster Training (Potentially): Fewer parameters typically mean fewer computations per training step, potentially 
% leading to faster epochs, although the overall training dynamic is complex.

% Improved Training Stability and Sample Efficiency:

% The shared base learns common low-level and mid-level features more effectively because it receives gradient 
% updates influenced by the learning objectives of all generators. It essentially gets a stronger, more averaged 
% learning signal for shared features.

% This shared learning can prevent individual generators from collapsing early or diverging drastically, potentially 
% leading to more stable training.

% Generators benefit from features learned via the experiences of other generators through the shared base.

% Knowledge Transfer: Useful features learned by the shared base (e.g., basic shapes, textures common to the dataset) 
% are immediately available to all generators, promoting faster learning of complex structures.

% Implicit Regularization: Sharing weights constrains the generators, acting as a form of regularization that might 
% prevent individual generators from overfitting noise or specific data points too quickly.

% Disadvantages of Weight Sharing:

% 

% Limited Diversity Potential: Because a significant portion of the network is shared, the fundamental feature 
% extraction process is common to all generators. The diversity is primarily introduced by the final few unshared 
% layers. This might impose a ceiling on the maximum achievable diversity compared to completely independent 
% generators, which could potentially learn radically different internal representations from scratch.

% Optimization Complexity: Training the shared parameters involves backpropagating gradients derived from multiple 
% generator heads, each with its adversarial loss and influenced by the diversity loss relative to others. Balancing 
% these potentially conflicting gradient signals flowing into the shared base can be tricky and might require careful 
% tuning of learning rates and the diversity weight (lambda). Poor tuning could lead to suboptimal learning in the 
% shared base.

% Architectural Constraint: The fixed structure (shared base + separate heads) might not be the optimal architecture 
% for every problem or dataset. Completely independent generators offer more flexibility in exploring different 
% architectures for each one.

% Potential Bottleneck: If the shared base fails to learn good representations or gets stuck in a poor local minimum, 
% it negatively impacts all generators simultaneously. With independent generators, there's a chance that at least 
% one might find a better solution path independently.

% In summary, the weight-sharing strategy in the original MAD-GAN paper is a clever design choice offering 
% significant efficiency and stability benefits, making it practical to train multiple generators. However, this 
% comes at the cost of potentially limiting the absolute maximum diversity achievable and introducing specific 
% optimization challenges compared to using fully independent generator networks. The choice depends on the trade-off 
% between efficiency, stability, and the desired level/type of diversity for a specific application.

% 

% 

% \cite{zhao2023gan}

% TODO: cite this again. they managed to apply dc gans to cifar10 

\newpage
